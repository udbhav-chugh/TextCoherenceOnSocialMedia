{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextCoherenceModel",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udbhav-chugh/TextCoherenceOnSocialMedia/blob/master/TextCoherenceModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga8-lENuFIcC"
      },
      "source": [
        "# This notebook contains the code for training and testing the text coherence model\n",
        "\n",
        "*   Note you must have previously run the notebook to preprocess and generate word embeddings for each of the datasets. This notebook trains and tests models for three datasets:\n",
        "  * One is the generated Social Media dataset which considers posts and comments as separate documents\n",
        "  * One is the generated Social Media dataset which considers posts and top 10 comments as single documents\n",
        "  * One is the Accidents Report dataset which is the standard dataset used for text coherence analysis.\n",
        "*   We train our model on all three datasets, and for each model, we test on all three sets. See the report for result analysis.\n",
        "* Ensure the word embeddings for the training data and testing data are present in your google drive. For our notebook, we kept a folder named nlp in google drive which had three sub-folders: separate, combined and accident. All three folders had 2 files each: the word embeddings for training and testing dataset.\n",
        "* Once the word embeddings are present in the required folder with names mentioned in the notebook, you can easily run the notebook.\n",
        "* It takes a few hours for training and testing to be complete.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAekMKkTqFHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4d542c5-86b1-4097-bbc7-cfc9a41da993"
      },
      "source": [
        "import inspect\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torch.autograd import Function, Variable\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from torch.optim import Adam, SGD\n",
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import math\n",
        "import pickle\n",
        "\n",
        "torch.manual_seed(191009)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f45c2109588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL_u6TqeG6eF"
      },
      "source": [
        "# The proposed architecture code to get coherence score for a window of 3 sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDvvuhVBRJOH"
      },
      "source": [
        "class coherenceModel(nn.Module):\n",
        "  def __init__(self):\n",
        "\n",
        "    super(coherenceModel, self).__init__()\n",
        "    #Layer 1\n",
        "    self.conv2DSent1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,50), stride=1, padding=(1, 0))\n",
        "    self.conv2DSent2 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,50), stride=1, padding=(1, 0))\n",
        "    self.conv2DSent3 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(3,50), stride=1, padding=(1, 0))\n",
        "\n",
        "    self.avgPool2DSent1 = nn.AdaptiveAvgPool1d(1)\n",
        "    self.avgPool2DSent2 = nn.AdaptiveAvgPool1d(1)\n",
        "    self.avgPool2DSent3 = nn.AdaptiveAvgPool1d(1)\n",
        "\n",
        "    self.hiddenLayer = nn.Conv1d(in_channels = 1, out_channels = 1, kernel_size=1,stride=1)\n",
        "\n",
        "    self.finalLayer = nn.AvgPool1d(kernel_size=48)\n",
        "    self.endSigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self,sentence1, sentence2, sentence3):\n",
        "    \n",
        "    #Layer 1\n",
        "    self.output2DSent1 = self.conv2DSent1(sentence1)\n",
        "    self.output2DSent2 = self.conv2DSent2(sentence2)\n",
        "    self.output2DSent3 = self.conv2DSent3(sentence3)\n",
        "\n",
        "    self.output1DSent1Squeezed = self.output2DSent1.squeeze(3)\n",
        "    self.output1DSent2Squeezed = self.output2DSent2.squeeze(3)\n",
        "    self.output1DSent3Squeezed = self.output2DSent3.squeeze(3)\n",
        "\n",
        "    self.outputPoolSent1 = self.avgPool2DSent1(self.output1DSent1Squeezed)\n",
        "    self.outputPoolSent2 = self.avgPool2DSent2(self.output1DSent2Squeezed)\n",
        "    self.outputPoolSent3 = self.avgPool2DSent3(self.output1DSent3Squeezed)\n",
        "\n",
        "    self.inputForJoinSent1 = self.outputPoolSent1.squeeze(2).unsqueeze(0)\n",
        "    self.inputForJoinSent2 = self.outputPoolSent2.squeeze(2).unsqueeze(0)\n",
        "    self.inputForJoinSent3 = self.outputPoolSent3.squeeze(2).unsqueeze(0)\n",
        "\n",
        "\n",
        "\n",
        "    self.neuralNetInput = torch.cat((self.inputForJoinSent1, self.inputForJoinSent2, self.inputForJoinSent3),2)\n",
        "\n",
        "    self.hiddenLayerOutput = self.hiddenLayer(self.neuralNetInput)\n",
        "\n",
        "    self.finalOutput = self.finalLayer(self.hiddenLayerOutput)\n",
        "\n",
        "    self.sigmoidInput = self.finalOutput.squeeze(0).squeeze(0)\n",
        "    self.finalProb = self.endSigmoid(self.sigmoidInput)\n",
        "    return self.finalProb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lv5NEksrHE8h"
      },
      "source": [
        "# Code to train the model and backpropogate based on the loss defined in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wssjZsncCxUd"
      },
      "source": [
        "\"\"\"# Training the model \"\"\"\n",
        "\n",
        "\n",
        "def customLoss(probTensor, outVal):\n",
        "  if outVal == 1:\n",
        "    logProbTensor = torch.log(probTensor)\n",
        "    logSumProbTensor = torch.sum(logProbTensor)\n",
        "    finalLoss = torch.mul(logSumProbTensor,-1)\n",
        "    return finalLoss\n",
        "  else:\n",
        "    oneTensor = torch.ones(1).to(device)\n",
        "    probTempTensor = torch.add(oneTensor, probTensor, alpha = -1)\n",
        "    logProbTensor = torch.log(probTempTensor)\n",
        "    logSumProbTensor = torch.sum(logProbTensor)\n",
        "    finalLoss = torch.mul(logSumProbTensor,-1)\n",
        "    return finalLoss\n",
        "\n",
        "def train_model(model, model_name, optimizer, fileName, num_epochs=25):\n",
        "    since = time.time()\n",
        "    print(\"Start Training\")\n",
        "    model_dir = '/content/ckpts_'+model_name\n",
        "    if not os.path.exists(model_dir):\n",
        "      os.makedirs(model_dir)\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "        epoch_loss = 0.0\n",
        "        # curPostProb = Variable(torch.ones(1).to(device),requires_grad=True)\n",
        "        # curPostProb = torch.ones(1).to(device)\n",
        "        for post in fileName:\n",
        "          tempPost = post[0]\n",
        "          outVal = int(post[1])\n",
        "          model.train()  # Set model to training mode\n",
        "          inputSize = len(tempPost)\n",
        "          for i in range(0,inputSize-2):\n",
        "            sentence1 = tempPost[i]\n",
        "            sentence2 = tempPost[i+1]\n",
        "            sentence3 = tempPost[i+2]\n",
        "\n",
        "            sentence1Tensor = torch.from_numpy(sentence1)\n",
        "            sentence1Tensor = torch.unsqueeze(sentence1Tensor,0)\n",
        "            sentence1Tensor = torch.unsqueeze(sentence1Tensor,0).to(device)\n",
        "\n",
        "            sentence2Tensor = torch.from_numpy(sentence2)\n",
        "            sentence2Tensor = torch.unsqueeze(sentence2Tensor,0)\n",
        "            sentence2Tensor = torch.unsqueeze(sentence2Tensor,0).to(device)\n",
        "\n",
        "            sentence3Tensor = torch.from_numpy(sentence3)\n",
        "            sentence3Tensor = torch.unsqueeze(sentence3Tensor,0)\n",
        "            sentence3Tensor = torch.unsqueeze(sentence3Tensor,0).to(device)\n",
        "            probCur = model(sentence1Tensor,sentence2Tensor,sentence3Tensor)\n",
        "            detachTensor = probCur.detach().clone()\n",
        "            # curPostProb = torch.cat((curPostProb, detachTensor),0)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "            loss = customLoss(probCur, outVal)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "\n",
        "        print('Epoch Loss: ', epoch_loss)\n",
        "        states = {\n",
        "            'model_state': model.state_dict(),\n",
        "            'optim_state': optimizer.state_dict(),\n",
        "            'ep_loss': epoch_loss\n",
        "        }\n",
        "        torch.save(states, os.path.join(model_dir, 'net_'+str(epoch)+'.pth'))\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "\n",
        "    return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fpWAPQWHKBC"
      },
      "source": [
        "# Code to run test on the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63zKIQGDfUNv"
      },
      "source": [
        "def test_model(model, model_name, index, testFile):\n",
        "    since = time.time()\n",
        "    print(\"Start Testing\")\n",
        "    model.eval()\n",
        "\n",
        "    lastepoch=299\n",
        "    model_dir = '/content/ckpts_'+model_name\n",
        "    checkpoint = torch.load(os.path.join(model_dir,'net_'+ str(lastepoch)+\".pth\"))\n",
        "    model.load_state_dict(checkpoint['model_state'])\n",
        "\n",
        "    scores = []\n",
        "    for post in testFile:\n",
        "      tempPost = post[index]\n",
        "      inputSize = len(tempPost)\n",
        "      score = 1\n",
        "      for i in range(0,inputSize-2):\n",
        "        sentence1 = tempPost[i]\n",
        "        sentence2 = tempPost[i+1]\n",
        "        sentence3 = tempPost[i+2]\n",
        "\n",
        "        sentence1Tensor = torch.from_numpy(sentence1)\n",
        "        sentence1Tensor = torch.unsqueeze(sentence1Tensor,0)\n",
        "        sentence1Tensor = torch.unsqueeze(sentence1Tensor,0).to(device)\n",
        "\n",
        "        sentence2Tensor = torch.from_numpy(sentence2)\n",
        "        sentence2Tensor = torch.unsqueeze(sentence2Tensor,0)\n",
        "        sentence2Tensor = torch.unsqueeze(sentence2Tensor,0).to(device)\n",
        "\n",
        "        sentence3Tensor = torch.from_numpy(sentence3)\n",
        "        sentence3Tensor = torch.unsqueeze(sentence3Tensor,0)\n",
        "        sentence3Tensor = torch.unsqueeze(sentence3Tensor,0).to(device)\n",
        "        probCur = model(sentence1Tensor,sentence2Tensor,sentence3Tensor)\n",
        "        detachTensor = probCur.detach().clone()\n",
        "        score *= (probCur.item())\n",
        "      scores.append(score)\n",
        "    \n",
        "    return scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhEf7hZcHOLH"
      },
      "source": [
        "# Training dataset1: Social Media dataset which considers posts and comments as separate documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUxrtBpPR0N8"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/separate/separate_train_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  separate_train_embeddings = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Tp5ZonGRMcF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8b5b6a4-7cbf-4847-83fa-2bb68951a1b4"
      },
      "source": [
        "# if __name__ == \"__main__\":\n",
        "model = coherenceModel()\n",
        "_lr = 2e-4\n",
        "optimizer = Adam(model.parameters(), lr=_lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Start\")\n",
        "model_ft = train_model(model, \"coherenceModelSeparate\", optimizer, separate_train_embeddings, num_epochs=300)\n",
        "print(\"End\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Start Training\n",
            "Epoch 0/299\n",
            "----------\n",
            "Epoch Loss:  6082.253101885319\n",
            "\n",
            "Epoch 1/299\n",
            "----------\n",
            "Epoch Loss:  5927.7052547335625\n",
            "\n",
            "Epoch 2/299\n",
            "----------\n",
            "Epoch Loss:  5925.112095355988\n",
            "\n",
            "Epoch 3/299\n",
            "----------\n",
            "Epoch Loss:  5923.072252601385\n",
            "\n",
            "Epoch 4/299\n",
            "----------\n",
            "Epoch Loss:  5921.2936980724335\n",
            "\n",
            "Epoch 5/299\n",
            "----------\n",
            "Epoch Loss:  5919.718302875757\n",
            "\n",
            "Epoch 6/299\n",
            "----------\n",
            "Epoch Loss:  5918.3052043914795\n",
            "\n",
            "Epoch 7/299\n",
            "----------\n",
            "Epoch Loss:  5917.02326208353\n",
            "\n",
            "Epoch 8/299\n",
            "----------\n",
            "Epoch Loss:  5915.8499675393105\n",
            "\n",
            "Epoch 9/299\n",
            "----------\n",
            "Epoch Loss:  5914.768180042505\n",
            "\n",
            "Epoch 10/299\n",
            "----------\n",
            "Epoch Loss:  5913.764505922794\n",
            "\n",
            "Epoch 11/299\n",
            "----------\n",
            "Epoch Loss:  5912.827797174454\n",
            "\n",
            "Epoch 12/299\n",
            "----------\n",
            "Epoch Loss:  5911.949794650078\n",
            "\n",
            "Epoch 13/299\n",
            "----------\n",
            "Epoch Loss:  5911.123561650515\n",
            "\n",
            "Epoch 14/299\n",
            "----------\n",
            "Epoch Loss:  5910.343365341425\n",
            "\n",
            "Epoch 15/299\n",
            "----------\n",
            "Epoch Loss:  5909.60381641984\n",
            "\n",
            "Epoch 16/299\n",
            "----------\n",
            "Epoch Loss:  5908.9012697041035\n",
            "\n",
            "Epoch 17/299\n",
            "----------\n",
            "Epoch Loss:  5908.231657922268\n",
            "\n",
            "Epoch 18/299\n",
            "----------\n",
            "Epoch Loss:  5907.592053771019\n",
            "\n",
            "Epoch 19/299\n",
            "----------\n",
            "Epoch Loss:  5906.980006963015\n",
            "\n",
            "Epoch 20/299\n",
            "----------\n",
            "Epoch Loss:  5906.39218211174\n",
            "\n",
            "Epoch 21/299\n",
            "----------\n",
            "Epoch Loss:  5905.826889842749\n",
            "\n",
            "Epoch 22/299\n",
            "----------\n",
            "Epoch Loss:  5905.282142341137\n",
            "\n",
            "Epoch 23/299\n",
            "----------\n",
            "Epoch Loss:  5904.756352782249\n",
            "\n",
            "Epoch 24/299\n",
            "----------\n",
            "Epoch Loss:  5904.247637480497\n",
            "\n",
            "Epoch 25/299\n",
            "----------\n",
            "Epoch Loss:  5903.755226522684\n",
            "\n",
            "Epoch 26/299\n",
            "----------\n",
            "Epoch Loss:  5903.277000665665\n",
            "\n",
            "Epoch 27/299\n",
            "----------\n",
            "Epoch Loss:  5902.811799407005\n",
            "\n",
            "Epoch 28/299\n",
            "----------\n",
            "Epoch Loss:  5902.359410554171\n",
            "\n",
            "Epoch 29/299\n",
            "----------\n",
            "Epoch Loss:  5901.91849565506\n",
            "\n",
            "Epoch 30/299\n",
            "----------\n",
            "Epoch Loss:  5901.488155126572\n",
            "\n",
            "Epoch 31/299\n",
            "----------\n",
            "Epoch Loss:  5901.068015992641\n",
            "\n",
            "Epoch 32/299\n",
            "----------\n",
            "Epoch Loss:  5900.65698698163\n",
            "\n",
            "Epoch 33/299\n",
            "----------\n",
            "Epoch Loss:  5900.2547143399715\n",
            "\n",
            "Epoch 34/299\n",
            "----------\n",
            "Epoch Loss:  5899.860262513161\n",
            "\n",
            "Epoch 35/299\n",
            "----------\n",
            "Epoch Loss:  5899.473933398724\n",
            "\n",
            "Epoch 36/299\n",
            "----------\n",
            "Epoch Loss:  5899.09450507164\n",
            "\n",
            "Epoch 37/299\n",
            "----------\n",
            "Epoch Loss:  5898.721841543913\n",
            "\n",
            "Epoch 38/299\n",
            "----------\n",
            "Epoch Loss:  5898.35574221611\n",
            "\n",
            "Epoch 39/299\n",
            "----------\n",
            "Epoch Loss:  5897.995587915182\n",
            "\n",
            "Epoch 40/299\n",
            "----------\n",
            "Epoch Loss:  5897.641315788031\n",
            "\n",
            "Epoch 41/299\n",
            "----------\n",
            "Epoch Loss:  5897.2928220927715\n",
            "\n",
            "Epoch 42/299\n",
            "----------\n",
            "Epoch Loss:  5896.9488960802555\n",
            "\n",
            "Epoch 43/299\n",
            "----------\n",
            "Epoch Loss:  5896.610344916582\n",
            "\n",
            "Epoch 44/299\n",
            "----------\n",
            "Epoch Loss:  5896.277147084475\n",
            "\n",
            "Epoch 45/299\n",
            "----------\n",
            "Epoch Loss:  5895.947951287031\n",
            "\n",
            "Epoch 46/299\n",
            "----------\n",
            "Epoch Loss:  5895.623355418444\n",
            "\n",
            "Epoch 47/299\n",
            "----------\n",
            "Epoch Loss:  5895.303163349628\n",
            "\n",
            "Epoch 48/299\n",
            "----------\n",
            "Epoch Loss:  5894.987151145935\n",
            "\n",
            "Epoch 49/299\n",
            "----------\n",
            "Epoch Loss:  5894.675278156996\n",
            "\n",
            "Epoch 50/299\n",
            "----------\n",
            "Epoch Loss:  5894.366913974285\n",
            "\n",
            "Epoch 51/299\n",
            "----------\n",
            "Epoch Loss:  5894.062737524509\n",
            "\n",
            "Epoch 52/299\n",
            "----------\n",
            "Epoch Loss:  5893.761918216944\n",
            "\n",
            "Epoch 53/299\n",
            "----------\n",
            "Epoch Loss:  5893.464774340391\n",
            "\n",
            "Epoch 54/299\n",
            "----------\n",
            "Epoch Loss:  5893.171262562275\n",
            "\n",
            "Epoch 55/299\n",
            "----------\n",
            "Epoch Loss:  5892.881207108498\n",
            "\n",
            "Epoch 56/299\n",
            "----------\n",
            "Epoch Loss:  5892.594492584467\n",
            "\n",
            "Epoch 57/299\n",
            "----------\n",
            "Epoch Loss:  5892.31088206172\n",
            "\n",
            "Epoch 58/299\n",
            "----------\n",
            "Epoch Loss:  5892.030257463455\n",
            "\n",
            "Epoch 59/299\n",
            "----------\n",
            "Epoch Loss:  5891.7532614171505\n",
            "\n",
            "Epoch 60/299\n",
            "----------\n",
            "Epoch Loss:  5891.47909155488\n",
            "\n",
            "Epoch 61/299\n",
            "----------\n",
            "Epoch Loss:  5891.208254694939\n",
            "\n",
            "Epoch 62/299\n",
            "----------\n",
            "Epoch Loss:  5890.940238714218\n",
            "\n",
            "Epoch 63/299\n",
            "----------\n",
            "Epoch Loss:  5890.6752406954765\n",
            "\n",
            "Epoch 64/299\n",
            "----------\n",
            "Epoch Loss:  5890.412911564112\n",
            "\n",
            "Epoch 65/299\n",
            "----------\n",
            "Epoch Loss:  5890.153655767441\n",
            "\n",
            "Epoch 66/299\n",
            "----------\n",
            "Epoch Loss:  5889.897239208221\n",
            "\n",
            "Epoch 67/299\n",
            "----------\n",
            "Epoch Loss:  5889.642969101667\n",
            "\n",
            "Epoch 68/299\n",
            "----------\n",
            "Epoch Loss:  5889.392109036446\n",
            "\n",
            "Epoch 69/299\n",
            "----------\n",
            "Epoch Loss:  5889.144209623337\n",
            "\n",
            "Epoch 70/299\n",
            "----------\n",
            "Epoch Loss:  5888.898862957954\n",
            "\n",
            "Epoch 71/299\n",
            "----------\n",
            "Epoch Loss:  5888.655900448561\n",
            "\n",
            "Epoch 72/299\n",
            "----------\n",
            "Epoch Loss:  5888.415803283453\n",
            "\n",
            "Epoch 73/299\n",
            "----------\n",
            "Epoch Loss:  5888.178154468536\n",
            "\n",
            "Epoch 74/299\n",
            "----------\n",
            "Epoch Loss:  5887.9432600438595\n",
            "\n",
            "Epoch 75/299\n",
            "----------\n",
            "Epoch Loss:  5887.7108580470085\n",
            "\n",
            "Epoch 76/299\n",
            "----------\n",
            "Epoch Loss:  5887.481019437313\n",
            "\n",
            "Epoch 77/299\n",
            "----------\n",
            "Epoch Loss:  5887.253606587648\n",
            "\n",
            "Epoch 78/299\n",
            "----------\n",
            "Epoch Loss:  5887.029083132744\n",
            "\n",
            "Epoch 79/299\n",
            "----------\n",
            "Epoch Loss:  5886.806788831949\n",
            "\n",
            "Epoch 80/299\n",
            "----------\n",
            "Epoch Loss:  5886.586928844452\n",
            "\n",
            "Epoch 81/299\n",
            "----------\n",
            "Epoch Loss:  5886.369985163212\n",
            "\n",
            "Epoch 82/299\n",
            "----------\n",
            "Epoch Loss:  5886.155276417732\n",
            "\n",
            "Epoch 83/299\n",
            "----------\n",
            "Epoch Loss:  5885.9426447451115\n",
            "\n",
            "Epoch 84/299\n",
            "----------\n",
            "Epoch Loss:  5885.732653290033\n",
            "\n",
            "Epoch 85/299\n",
            "----------\n",
            "Epoch Loss:  5885.524954795837\n",
            "\n",
            "Epoch 86/299\n",
            "----------\n",
            "Epoch Loss:  5885.319665163755\n",
            "\n",
            "Epoch 87/299\n",
            "----------\n",
            "Epoch Loss:  5885.116904318333\n",
            "\n",
            "Epoch 88/299\n",
            "----------\n",
            "Epoch Loss:  5884.916164010763\n",
            "\n",
            "Epoch 89/299\n",
            "----------\n",
            "Epoch Loss:  5884.717520356178\n",
            "\n",
            "Epoch 90/299\n",
            "----------\n",
            "Epoch Loss:  5884.521808087826\n",
            "\n",
            "Epoch 91/299\n",
            "----------\n",
            "Epoch Loss:  5884.328094810247\n",
            "\n",
            "Epoch 92/299\n",
            "----------\n",
            "Epoch Loss:  5884.136614769697\n",
            "\n",
            "Epoch 93/299\n",
            "----------\n",
            "Epoch Loss:  5883.947245687246\n",
            "\n",
            "Epoch 94/299\n",
            "----------\n",
            "Epoch Loss:  5883.760099977255\n",
            "\n",
            "Epoch 95/299\n",
            "----------\n",
            "Epoch Loss:  5883.575213193893\n",
            "\n",
            "Epoch 96/299\n",
            "----------\n",
            "Epoch Loss:  5883.392491042614\n",
            "\n",
            "Epoch 97/299\n",
            "----------\n",
            "Epoch Loss:  5883.211965560913\n",
            "\n",
            "Epoch 98/299\n",
            "----------\n",
            "Epoch Loss:  5883.0335329174995\n",
            "\n",
            "Epoch 99/299\n",
            "----------\n",
            "Epoch Loss:  5882.8570576012135\n",
            "\n",
            "Epoch 100/299\n",
            "----------\n",
            "Epoch Loss:  5882.683058053255\n",
            "\n",
            "Epoch 101/299\n",
            "----------\n",
            "Epoch Loss:  5882.511038959026\n",
            "\n",
            "Epoch 102/299\n",
            "----------\n",
            "Epoch Loss:  5882.340939462185\n",
            "\n",
            "Epoch 103/299\n",
            "----------\n",
            "Epoch Loss:  5882.172541230917\n",
            "\n",
            "Epoch 104/299\n",
            "----------\n",
            "Epoch Loss:  5882.006461918354\n",
            "\n",
            "Epoch 105/299\n",
            "----------\n",
            "Epoch Loss:  5881.842315167189\n",
            "\n",
            "Epoch 106/299\n",
            "----------\n",
            "Epoch Loss:  5881.68075761199\n",
            "\n",
            "Epoch 107/299\n",
            "----------\n",
            "Epoch Loss:  5881.520533636212\n",
            "\n",
            "Epoch 108/299\n",
            "----------\n",
            "Epoch Loss:  5881.362345322967\n",
            "\n",
            "Epoch 109/299\n",
            "----------\n",
            "Epoch Loss:  5881.205968916416\n",
            "\n",
            "Epoch 110/299\n",
            "----------\n",
            "Epoch Loss:  5881.051981389523\n",
            "\n",
            "Epoch 111/299\n",
            "----------\n",
            "Epoch Loss:  5880.8994535803795\n",
            "\n",
            "Epoch 112/299\n",
            "----------\n",
            "Epoch Loss:  5880.749230146408\n",
            "\n",
            "Epoch 113/299\n",
            "----------\n",
            "Epoch Loss:  5880.6004037112\n",
            "\n",
            "Epoch 114/299\n",
            "----------\n",
            "Epoch Loss:  5880.453389286995\n",
            "\n",
            "Epoch 115/299\n",
            "----------\n",
            "Epoch Loss:  5880.308454617858\n",
            "\n",
            "Epoch 116/299\n",
            "----------\n",
            "Epoch Loss:  5880.165508195758\n",
            "\n",
            "Epoch 117/299\n",
            "----------\n",
            "Epoch Loss:  5880.023692429066\n",
            "\n",
            "Epoch 118/299\n",
            "----------\n",
            "Epoch Loss:  5879.883907407522\n",
            "\n",
            "Epoch 119/299\n",
            "----------\n",
            "Epoch Loss:  5879.7458423525095\n",
            "\n",
            "Epoch 120/299\n",
            "----------\n",
            "Epoch Loss:  5879.609243109822\n",
            "\n",
            "Epoch 121/299\n",
            "----------\n",
            "Epoch Loss:  5879.474875047803\n",
            "\n",
            "Epoch 122/299\n",
            "----------\n",
            "Epoch Loss:  5879.341705664992\n",
            "\n",
            "Epoch 123/299\n",
            "----------\n",
            "Epoch Loss:  5879.210462406278\n",
            "\n",
            "Epoch 124/299\n",
            "----------\n",
            "Epoch Loss:  5879.080685809255\n",
            "\n",
            "Epoch 125/299\n",
            "----------\n",
            "Epoch Loss:  5878.952268764377\n",
            "\n",
            "Epoch 126/299\n",
            "----------\n",
            "Epoch Loss:  5878.826376363635\n",
            "\n",
            "Epoch 127/299\n",
            "----------\n",
            "Epoch Loss:  5878.701629206538\n",
            "\n",
            "Epoch 128/299\n",
            "----------\n",
            "Epoch Loss:  5878.578044787049\n",
            "\n",
            "Epoch 129/299\n",
            "----------\n",
            "Epoch Loss:  5878.456473365426\n",
            "\n",
            "Epoch 130/299\n",
            "----------\n",
            "Epoch Loss:  5878.336318463087\n",
            "\n",
            "Epoch 131/299\n",
            "----------\n",
            "Epoch Loss:  5878.217574954033\n",
            "\n",
            "Epoch 132/299\n",
            "----------\n",
            "Epoch Loss:  5878.100120142102\n",
            "\n",
            "Epoch 133/299\n",
            "----------\n",
            "Epoch Loss:  5877.984456136823\n",
            "\n",
            "Epoch 134/299\n",
            "----------\n",
            "Epoch Loss:  5877.869930654764\n",
            "\n",
            "Epoch 135/299\n",
            "----------\n",
            "Epoch Loss:  5877.756774365902\n",
            "\n",
            "Epoch 136/299\n",
            "----------\n",
            "Epoch Loss:  5877.645434945822\n",
            "\n",
            "Epoch 137/299\n",
            "----------\n",
            "Epoch Loss:  5877.535567954183\n",
            "\n",
            "Epoch 138/299\n",
            "----------\n",
            "Epoch Loss:  5877.426627665758\n",
            "\n",
            "Epoch 139/299\n",
            "----------\n",
            "Epoch Loss:  5877.319352373481\n",
            "\n",
            "Epoch 140/299\n",
            "----------\n",
            "Epoch Loss:  5877.213517069817\n",
            "\n",
            "Epoch 141/299\n",
            "----------\n",
            "Epoch Loss:  5877.108943149447\n",
            "\n",
            "Epoch 142/299\n",
            "----------\n",
            "Epoch Loss:  5877.005566224456\n",
            "\n",
            "Epoch 143/299\n",
            "----------\n",
            "Epoch Loss:  5876.902976974845\n",
            "\n",
            "Epoch 144/299\n",
            "----------\n",
            "Epoch Loss:  5876.802473425865\n",
            "\n",
            "Epoch 145/299\n",
            "----------\n",
            "Epoch Loss:  5876.703144147992\n",
            "\n",
            "Epoch 146/299\n",
            "----------\n",
            "Epoch Loss:  5876.604772508144\n",
            "\n",
            "Epoch 147/299\n",
            "----------\n",
            "Epoch Loss:  5876.507830068469\n",
            "\n",
            "Epoch 148/299\n",
            "----------\n",
            "Epoch Loss:  5876.4118829369545\n",
            "\n",
            "Epoch 149/299\n",
            "----------\n",
            "Epoch Loss:  5876.317610099912\n",
            "\n",
            "Epoch 150/299\n",
            "----------\n",
            "Epoch Loss:  5876.223771318793\n",
            "\n",
            "Epoch 151/299\n",
            "----------\n",
            "Epoch Loss:  5876.131996408105\n",
            "\n",
            "Epoch 152/299\n",
            "----------\n",
            "Epoch Loss:  5876.040690556169\n",
            "\n",
            "Epoch 153/299\n",
            "----------\n",
            "Epoch Loss:  5875.950833365321\n",
            "\n",
            "Epoch 154/299\n",
            "----------\n",
            "Epoch Loss:  5875.861874371767\n",
            "\n",
            "Epoch 155/299\n",
            "----------\n",
            "Epoch Loss:  5875.774283096194\n",
            "\n",
            "Epoch 156/299\n",
            "----------\n",
            "Epoch Loss:  5875.68810005486\n",
            "\n",
            "Epoch 157/299\n",
            "----------\n",
            "Epoch Loss:  5875.602173745632\n",
            "\n",
            "Epoch 158/299\n",
            "----------\n",
            "Epoch Loss:  5875.518092915416\n",
            "\n",
            "Epoch 159/299\n",
            "----------\n",
            "Epoch Loss:  5875.434443116188\n",
            "\n",
            "Epoch 160/299\n",
            "----------\n",
            "Epoch Loss:  5875.352306500077\n",
            "\n",
            "Epoch 161/299\n",
            "----------\n",
            "Epoch Loss:  5875.271066725254\n",
            "\n",
            "Epoch 162/299\n",
            "----------\n",
            "Epoch Loss:  5875.190800458193\n",
            "\n",
            "Epoch 163/299\n",
            "----------\n",
            "Epoch Loss:  5875.111693024635\n",
            "\n",
            "Epoch 164/299\n",
            "----------\n",
            "Epoch Loss:  5875.03354729712\n",
            "\n",
            "Epoch 165/299\n",
            "----------\n",
            "Epoch Loss:  5874.956158518791\n",
            "\n",
            "Epoch 166/299\n",
            "----------\n",
            "Epoch Loss:  5874.879903912544\n",
            "\n",
            "Epoch 167/299\n",
            "----------\n",
            "Epoch Loss:  5874.804479584098\n",
            "\n",
            "Epoch 168/299\n",
            "----------\n",
            "Epoch Loss:  5874.730290591717\n",
            "\n",
            "Epoch 169/299\n",
            "----------\n",
            "Epoch Loss:  5874.656980738044\n",
            "\n",
            "Epoch 170/299\n",
            "----------\n",
            "Epoch Loss:  5874.584110200405\n",
            "\n",
            "Epoch 171/299\n",
            "----------\n",
            "Epoch Loss:  5874.51293104887\n",
            "\n",
            "Epoch 172/299\n",
            "----------\n",
            "Epoch Loss:  5874.442327603698\n",
            "\n",
            "Epoch 173/299\n",
            "----------\n",
            "Epoch Loss:  5874.372296139598\n",
            "\n",
            "Epoch 174/299\n",
            "----------\n",
            "Epoch Loss:  5874.303576886654\n",
            "\n",
            "Epoch 175/299\n",
            "----------\n",
            "Epoch Loss:  5874.235590353608\n",
            "\n",
            "Epoch 176/299\n",
            "----------\n",
            "Epoch Loss:  5874.168388336897\n",
            "\n",
            "Epoch 177/299\n",
            "----------\n",
            "Epoch Loss:  5874.101988375187\n",
            "\n",
            "Epoch 178/299\n",
            "----------\n",
            "Epoch Loss:  5874.036657258868\n",
            "\n",
            "Epoch 179/299\n",
            "----------\n",
            "Epoch Loss:  5873.97218413651\n",
            "\n",
            "Epoch 180/299\n",
            "----------\n",
            "Epoch Loss:  5873.908267304301\n",
            "\n",
            "Epoch 181/299\n",
            "----------\n",
            "Epoch Loss:  5873.8453291505575\n",
            "\n",
            "Epoch 182/299\n",
            "----------\n",
            "Epoch Loss:  5873.783425629139\n",
            "\n",
            "Epoch 183/299\n",
            "----------\n",
            "Epoch Loss:  5873.721647918224\n",
            "\n",
            "Epoch 184/299\n",
            "----------\n",
            "Epoch Loss:  5873.660674765706\n",
            "\n",
            "Epoch 185/299\n",
            "----------\n",
            "Epoch Loss:  5873.601141601801\n",
            "\n",
            "Epoch 186/299\n",
            "----------\n",
            "Epoch Loss:  5873.542112514377\n",
            "\n",
            "Epoch 187/299\n",
            "----------\n",
            "Epoch Loss:  5873.483688399196\n",
            "\n",
            "Epoch 188/299\n",
            "----------\n",
            "Epoch Loss:  5873.42598554492\n",
            "\n",
            "Epoch 189/299\n",
            "----------\n",
            "Epoch Loss:  5873.36925457418\n",
            "\n",
            "Epoch 190/299\n",
            "----------\n",
            "Epoch Loss:  5873.313070356846\n",
            "\n",
            "Epoch 191/299\n",
            "----------\n",
            "Epoch Loss:  5873.258081391454\n",
            "\n",
            "Epoch 192/299\n",
            "----------\n",
            "Epoch Loss:  5873.20299051702\n",
            "\n",
            "Epoch 193/299\n",
            "----------\n",
            "Epoch Loss:  5873.149337366223\n",
            "\n",
            "Epoch 194/299\n",
            "----------\n",
            "Epoch Loss:  5873.096144706011\n",
            "\n",
            "Epoch 195/299\n",
            "----------\n",
            "Epoch Loss:  5873.043457478285\n",
            "\n",
            "Epoch 196/299\n",
            "----------\n",
            "Epoch Loss:  5872.991276070476\n",
            "\n",
            "Epoch 197/299\n",
            "----------\n",
            "Epoch Loss:  5872.94004496932\n",
            "\n",
            "Epoch 198/299\n",
            "----------\n",
            "Epoch Loss:  5872.889351323247\n",
            "\n",
            "Epoch 199/299\n",
            "----------\n",
            "Epoch Loss:  5872.839568912983\n",
            "\n",
            "Epoch 200/299\n",
            "----------\n",
            "Epoch Loss:  5872.790151536465\n",
            "\n",
            "Epoch 201/299\n",
            "----------\n",
            "Epoch Loss:  5872.741361826658\n",
            "\n",
            "Epoch 202/299\n",
            "----------\n",
            "Epoch Loss:  5872.693571820855\n",
            "\n",
            "Epoch 203/299\n",
            "----------\n",
            "Epoch Loss:  5872.646173655987\n",
            "\n",
            "Epoch 204/299\n",
            "----------\n",
            "Epoch Loss:  5872.599404111505\n",
            "\n",
            "Epoch 205/299\n",
            "----------\n",
            "Epoch Loss:  5872.553011447191\n",
            "\n",
            "Epoch 206/299\n",
            "----------\n",
            "Epoch Loss:  5872.507516801357\n",
            "\n",
            "Epoch 207/299\n",
            "----------\n",
            "Epoch Loss:  5872.46221151948\n",
            "\n",
            "Epoch 208/299\n",
            "----------\n",
            "Epoch Loss:  5872.418105378747\n",
            "\n",
            "Epoch 209/299\n",
            "----------\n",
            "Epoch Loss:  5872.374314278364\n",
            "\n",
            "Epoch 210/299\n",
            "----------\n",
            "Epoch Loss:  5872.330798014998\n",
            "\n",
            "Epoch 211/299\n",
            "----------\n",
            "Epoch Loss:  5872.288118511438\n",
            "\n",
            "Epoch 212/299\n",
            "----------\n",
            "Epoch Loss:  5872.245829612017\n",
            "\n",
            "Epoch 213/299\n",
            "----------\n",
            "Epoch Loss:  5872.204128339887\n",
            "\n",
            "Epoch 214/299\n",
            "----------\n",
            "Epoch Loss:  5872.1632589399815\n",
            "\n",
            "Epoch 215/299\n",
            "----------\n",
            "Epoch Loss:  5872.122842058539\n",
            "\n",
            "Epoch 216/299\n",
            "----------\n",
            "Epoch Loss:  5872.08280107379\n",
            "\n",
            "Epoch 217/299\n",
            "----------\n",
            "Epoch Loss:  5872.0435551553965\n",
            "\n",
            "Epoch 218/299\n",
            "----------\n",
            "Epoch Loss:  5872.0044848024845\n",
            "\n",
            "Epoch 219/299\n",
            "----------\n",
            "Epoch Loss:  5871.966285005212\n",
            "\n",
            "Epoch 220/299\n",
            "----------\n",
            "Epoch Loss:  5871.928294166923\n",
            "\n",
            "Epoch 221/299\n",
            "----------\n",
            "Epoch Loss:  5871.891074120998\n",
            "\n",
            "Epoch 222/299\n",
            "----------\n",
            "Epoch Loss:  5871.854076102376\n",
            "\n",
            "Epoch 223/299\n",
            "----------\n",
            "Epoch Loss:  5871.817527770996\n",
            "\n",
            "Epoch 224/299\n",
            "----------\n",
            "Epoch Loss:  5871.781638205051\n",
            "\n",
            "Epoch 225/299\n",
            "----------\n",
            "Epoch Loss:  5871.746071964502\n",
            "\n",
            "Epoch 226/299\n",
            "----------\n",
            "Epoch Loss:  5871.711290553212\n",
            "\n",
            "Epoch 227/299\n",
            "----------\n",
            "Epoch Loss:  5871.676764249802\n",
            "\n",
            "Epoch 228/299\n",
            "----------\n",
            "Epoch Loss:  5871.642592549324\n",
            "\n",
            "Epoch 229/299\n",
            "----------\n",
            "Epoch Loss:  5871.609438940883\n",
            "\n",
            "Epoch 230/299\n",
            "----------\n",
            "Epoch Loss:  5871.576296672225\n",
            "\n",
            "Epoch 231/299\n",
            "----------\n",
            "Epoch Loss:  5871.543548986316\n",
            "\n",
            "Epoch 232/299\n",
            "----------\n",
            "Epoch Loss:  5871.511291027069\n",
            "\n",
            "Epoch 233/299\n",
            "----------\n",
            "Epoch Loss:  5871.479547873139\n",
            "\n",
            "Epoch 234/299\n",
            "----------\n",
            "Epoch Loss:  5871.448175266385\n",
            "\n",
            "Epoch 235/299\n",
            "----------\n",
            "Epoch Loss:  5871.417422041297\n",
            "\n",
            "Epoch 236/299\n",
            "----------\n",
            "Epoch Loss:  5871.386931031942\n",
            "\n",
            "Epoch 237/299\n",
            "----------\n",
            "Epoch Loss:  5871.356637492776\n",
            "\n",
            "Epoch 238/299\n",
            "----------\n",
            "Epoch Loss:  5871.327038764954\n",
            "\n",
            "Epoch 239/299\n",
            "----------\n",
            "Epoch Loss:  5871.297688320279\n",
            "\n",
            "Epoch 240/299\n",
            "----------\n",
            "Epoch Loss:  5871.26897662878\n",
            "\n",
            "Epoch 241/299\n",
            "----------\n",
            "Epoch Loss:  5871.240520939231\n",
            "\n",
            "Epoch 242/299\n",
            "----------\n",
            "Epoch Loss:  5871.212626904249\n",
            "\n",
            "Epoch 243/299\n",
            "----------\n",
            "Epoch Loss:  5871.185007154942\n",
            "\n",
            "Epoch 244/299\n",
            "----------\n",
            "Epoch Loss:  5871.157860279083\n",
            "\n",
            "Epoch 245/299\n",
            "----------\n",
            "Epoch Loss:  5871.131235077977\n",
            "\n",
            "Epoch 246/299\n",
            "----------\n",
            "Epoch Loss:  5871.104785010219\n",
            "\n",
            "Epoch 247/299\n",
            "----------\n",
            "Epoch Loss:  5871.078365638852\n",
            "\n",
            "Epoch 248/299\n",
            "----------\n",
            "Epoch Loss:  5871.0527107566595\n",
            "\n",
            "Epoch 249/299\n",
            "----------\n",
            "Epoch Loss:  5871.027283847332\n",
            "\n",
            "Epoch 250/299\n",
            "----------\n",
            "Epoch Loss:  5871.002331897616\n",
            "\n",
            "Epoch 251/299\n",
            "----------\n",
            "Epoch Loss:  5870.977716982365\n",
            "\n",
            "Epoch 252/299\n",
            "----------\n",
            "Epoch Loss:  5870.953143447638\n",
            "\n",
            "Epoch 253/299\n",
            "----------\n",
            "Epoch Loss:  5870.929579690099\n",
            "\n",
            "Epoch 254/299\n",
            "----------\n",
            "Epoch Loss:  5870.906121492386\n",
            "\n",
            "Epoch 255/299\n",
            "----------\n",
            "Epoch Loss:  5870.882850497961\n",
            "\n",
            "Epoch 256/299\n",
            "----------\n",
            "Epoch Loss:  5870.860069945455\n",
            "\n",
            "Epoch 257/299\n",
            "----------\n",
            "Epoch Loss:  5870.837560757995\n",
            "\n",
            "Epoch 258/299\n",
            "----------\n",
            "Epoch Loss:  5870.81535628438\n",
            "\n",
            "Epoch 259/299\n",
            "----------\n",
            "Epoch Loss:  5870.793505251408\n",
            "\n",
            "Epoch 260/299\n",
            "----------\n",
            "Epoch Loss:  5870.772026106715\n",
            "\n",
            "Epoch 261/299\n",
            "----------\n",
            "Epoch Loss:  5870.750807136297\n",
            "\n",
            "Epoch 262/299\n",
            "----------\n",
            "Epoch Loss:  5870.729884073138\n",
            "\n",
            "Epoch 263/299\n",
            "----------\n",
            "Epoch Loss:  5870.709257945418\n",
            "\n",
            "Epoch 264/299\n",
            "----------\n",
            "Epoch Loss:  5870.688965916634\n",
            "\n",
            "Epoch 265/299\n",
            "----------\n",
            "Epoch Loss:  5870.6688024401665\n",
            "\n",
            "Epoch 266/299\n",
            "----------\n",
            "Epoch Loss:  5870.649354681373\n",
            "\n",
            "Epoch 267/299\n",
            "----------\n",
            "Epoch Loss:  5870.629986569285\n",
            "\n",
            "Epoch 268/299\n",
            "----------\n",
            "Epoch Loss:  5870.610796958208\n",
            "\n",
            "Epoch 269/299\n",
            "----------\n",
            "Epoch Loss:  5870.592297628522\n",
            "\n",
            "Epoch 270/299\n",
            "----------\n",
            "Epoch Loss:  5870.573616415262\n",
            "\n",
            "Epoch 271/299\n",
            "----------\n",
            "Epoch Loss:  5870.55573476851\n",
            "\n",
            "Epoch 272/299\n",
            "----------\n",
            "Epoch Loss:  5870.537945538759\n",
            "\n",
            "Epoch 273/299\n",
            "----------\n",
            "Epoch Loss:  5870.520354196429\n",
            "\n",
            "Epoch 274/299\n",
            "----------\n",
            "Epoch Loss:  5870.503068596125\n",
            "\n",
            "Epoch 275/299\n",
            "----------\n",
            "Epoch Loss:  5870.48596341908\n",
            "\n",
            "Epoch 276/299\n",
            "----------\n",
            "Epoch Loss:  5870.4692590385675\n",
            "\n",
            "Epoch 277/299\n",
            "----------\n",
            "Epoch Loss:  5870.452832043171\n",
            "\n",
            "Epoch 278/299\n",
            "----------\n",
            "Epoch Loss:  5870.436726629734\n",
            "\n",
            "Epoch 279/299\n",
            "----------\n",
            "Epoch Loss:  5870.420885935426\n",
            "\n",
            "Epoch 280/299\n",
            "----------\n",
            "Epoch Loss:  5870.405038699508\n",
            "\n",
            "Epoch 281/299\n",
            "----------\n",
            "Epoch Loss:  5870.38981744647\n",
            "\n",
            "Epoch 282/299\n",
            "----------\n",
            "Epoch Loss:  5870.374636262655\n",
            "\n",
            "Epoch 283/299\n",
            "----------\n",
            "Epoch Loss:  5870.359661564231\n",
            "\n",
            "Epoch 284/299\n",
            "----------\n",
            "Epoch Loss:  5870.3449794352055\n",
            "\n",
            "Epoch 285/299\n",
            "----------\n",
            "Epoch Loss:  5870.330700159073\n",
            "\n",
            "Epoch 286/299\n",
            "----------\n",
            "Epoch Loss:  5870.316538840532\n",
            "\n",
            "Epoch 287/299\n",
            "----------\n",
            "Epoch Loss:  5870.302944019437\n",
            "\n",
            "Epoch 288/299\n",
            "----------\n",
            "Epoch Loss:  5870.289213627577\n",
            "\n",
            "Epoch 289/299\n",
            "----------\n",
            "Epoch Loss:  5870.27577316761\n",
            "\n",
            "Epoch 290/299\n",
            "----------\n",
            "Epoch Loss:  5870.262867465615\n",
            "\n",
            "Epoch 291/299\n",
            "----------\n",
            "Epoch Loss:  5870.249799162149\n",
            "\n",
            "Epoch 292/299\n",
            "----------\n",
            "Epoch Loss:  5870.237136274576\n",
            "\n",
            "Epoch 293/299\n",
            "----------\n",
            "Epoch Loss:  5870.224678605795\n",
            "\n",
            "Epoch 294/299\n",
            "----------\n",
            "Epoch Loss:  5870.21252194047\n",
            "\n",
            "Epoch 295/299\n",
            "----------\n",
            "Epoch Loss:  5870.200640946627\n",
            "\n",
            "Epoch 296/299\n",
            "----------\n",
            "Epoch Loss:  5870.188821017742\n",
            "\n",
            "Epoch 297/299\n",
            "----------\n",
            "Epoch Loss:  5870.177250236273\n",
            "\n",
            "Epoch 298/299\n",
            "----------\n",
            "Epoch Loss:  5870.1659303456545\n",
            "\n",
            "Epoch 299/299\n",
            "----------\n",
            "Epoch Loss:  5870.154746845365\n",
            "\n",
            "Training complete in 133m 49s\n",
            "End\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mha0MpxaHY8X"
      },
      "source": [
        "# Testing model 1 on all three test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mdndWnx4iIGN",
        "outputId": "3993bb6b-980e-4b8e-cade-b68b1e53a55b"
      },
      "source": [
        "model = coherenceModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "coherenceModel(\n",
              "  (conv2DSent1): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent2): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent3): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (avgPool2DSent1): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent2): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent3): AdaptiveAvgPool1d(output_size=1)\n",
              "  (hiddenLayer): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
              "  (finalLayer): AvgPool1d(kernel_size=(48,), stride=(48,), padding=(0,))\n",
              "  (endSigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-vUJTDWyu4F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0f7f6c-7a57-486d-c599-aae1cd51ed5f"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/separate/separate_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  separate_test_embeddings = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelSeparate\",0, separate_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelSeparate\",1, separate_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  71.10714285714285\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PReKOkCmJbS5",
        "outputId": "91fcdbb6-9915-4894-ab94-bb8d044784ce"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/combined/combined_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  combined_test_embeddings = pickle.load(fp)\n",
        "  \n",
        "coherentScores = test_model(model, \"coherenceModelSeparate\",0, combined_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelSeparate\",1, combined_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  70.78571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjE_B4mYiNlG",
        "outputId": "452ff311-46e8-4785-f994-c5e7e5082705"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/accident/datasetNTSBTesting.txt\", \"rb\") as fp:   # Unpickling\n",
        "  datasetNTSBTesting = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelSeparate\",0, datasetNTSBTesting)\n",
        "incoherentScores = test_model(model, \"coherenceModelSeparate\",1, datasetNTSBTesting)\n",
        "\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  64.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw7kLbuyHrrX"
      },
      "source": [
        "# Training dataset2: Social Media dataset which considers posts and top 10 comments as combined document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0AnKqxhXg4Ww"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/combined/combined_train_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  combined_train_embeddings = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oV5xDC815vh5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd47fe66-aa10-491b-fcef-39992ee37618"
      },
      "source": [
        "model = coherenceModel()\n",
        "_lr = 2e-4\n",
        "optimizer = Adam(model.parameters(), lr=_lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Start\")\n",
        "model_ft = train_model(model, \"coherenceModelCombined\", optimizer, combined_train_embeddings, num_epochs=300)\n",
        "print(\"End\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Start Training\n",
            "Epoch 0/299\n",
            "----------\n",
            "Epoch Loss:  5567.785762965679\n",
            "\n",
            "Epoch 1/299\n",
            "----------\n",
            "Epoch Loss:  5561.648887813091\n",
            "\n",
            "Epoch 2/299\n",
            "----------\n",
            "Epoch Loss:  5561.022981405258\n",
            "\n",
            "Epoch 3/299\n",
            "----------\n",
            "Epoch Loss:  5560.647276878357\n",
            "\n",
            "Epoch 4/299\n",
            "----------\n",
            "Epoch Loss:  5560.299709022045\n",
            "\n",
            "Epoch 5/299\n",
            "----------\n",
            "Epoch Loss:  5559.959327697754\n",
            "\n",
            "Epoch 6/299\n",
            "----------\n",
            "Epoch Loss:  5559.621977746487\n",
            "\n",
            "Epoch 7/299\n",
            "----------\n",
            "Epoch Loss:  5559.286205351353\n",
            "\n",
            "Epoch 8/299\n",
            "----------\n",
            "Epoch Loss:  5558.951483666897\n",
            "\n",
            "Epoch 9/299\n",
            "----------\n",
            "Epoch Loss:  5558.618040502071\n",
            "\n",
            "Epoch 10/299\n",
            "----------\n",
            "Epoch Loss:  5558.28657323122\n",
            "\n",
            "Epoch 11/299\n",
            "----------\n",
            "Epoch Loss:  5557.9582443237305\n",
            "\n",
            "Epoch 12/299\n",
            "----------\n",
            "Epoch Loss:  5557.634288609028\n",
            "\n",
            "Epoch 13/299\n",
            "----------\n",
            "Epoch Loss:  5557.315944075584\n",
            "\n",
            "Epoch 14/299\n",
            "----------\n",
            "Epoch Loss:  5557.004436373711\n",
            "\n",
            "Epoch 15/299\n",
            "----------\n",
            "Epoch Loss:  5556.700870633125\n",
            "\n",
            "Epoch 16/299\n",
            "----------\n",
            "Epoch Loss:  5556.405989289284\n",
            "\n",
            "Epoch 17/299\n",
            "----------\n",
            "Epoch Loss:  5556.120392858982\n",
            "\n",
            "Epoch 18/299\n",
            "----------\n",
            "Epoch Loss:  5555.844564199448\n",
            "\n",
            "Epoch 19/299\n",
            "----------\n",
            "Epoch Loss:  5555.578655004501\n",
            "\n",
            "Epoch 20/299\n",
            "----------\n",
            "Epoch Loss:  5555.322656571865\n",
            "\n",
            "Epoch 21/299\n",
            "----------\n",
            "Epoch Loss:  5555.076517403126\n",
            "\n",
            "Epoch 22/299\n",
            "----------\n",
            "Epoch Loss:  5554.839966595173\n",
            "\n",
            "Epoch 23/299\n",
            "----------\n",
            "Epoch Loss:  5554.612786471844\n",
            "\n",
            "Epoch 24/299\n",
            "----------\n",
            "Epoch Loss:  5554.394650220871\n",
            "\n",
            "Epoch 25/299\n",
            "----------\n",
            "Epoch Loss:  5554.185103356838\n",
            "\n",
            "Epoch 26/299\n",
            "----------\n",
            "Epoch Loss:  5553.983753025532\n",
            "\n",
            "Epoch 27/299\n",
            "----------\n",
            "Epoch Loss:  5553.790232479572\n",
            "\n",
            "Epoch 28/299\n",
            "----------\n",
            "Epoch Loss:  5553.604087710381\n",
            "\n",
            "Epoch 29/299\n",
            "----------\n",
            "Epoch Loss:  5553.424948513508\n",
            "\n",
            "Epoch 30/299\n",
            "----------\n",
            "Epoch Loss:  5553.252388715744\n",
            "\n",
            "Epoch 31/299\n",
            "----------\n",
            "Epoch Loss:  5553.086103141308\n",
            "\n",
            "Epoch 32/299\n",
            "----------\n",
            "Epoch Loss:  5552.92570066452\n",
            "\n",
            "Epoch 33/299\n",
            "----------\n",
            "Epoch Loss:  5552.770868241787\n",
            "\n",
            "Epoch 34/299\n",
            "----------\n",
            "Epoch Loss:  5552.621263086796\n",
            "\n",
            "Epoch 35/299\n",
            "----------\n",
            "Epoch Loss:  5552.476601839066\n",
            "\n",
            "Epoch 36/299\n",
            "----------\n",
            "Epoch Loss:  5552.336587548256\n",
            "\n",
            "Epoch 37/299\n",
            "----------\n",
            "Epoch Loss:  5552.201022088528\n",
            "\n",
            "Epoch 38/299\n",
            "----------\n",
            "Epoch Loss:  5552.069624602795\n",
            "\n",
            "Epoch 39/299\n",
            "----------\n",
            "Epoch Loss:  5551.942135453224\n",
            "\n",
            "Epoch 40/299\n",
            "----------\n",
            "Epoch Loss:  5551.818359017372\n",
            "\n",
            "Epoch 41/299\n",
            "----------\n",
            "Epoch Loss:  5551.698124945164\n",
            "\n",
            "Epoch 42/299\n",
            "----------\n",
            "Epoch Loss:  5551.581254065037\n",
            "\n",
            "Epoch 43/299\n",
            "----------\n",
            "Epoch Loss:  5551.467497050762\n",
            "\n",
            "Epoch 44/299\n",
            "----------\n",
            "Epoch Loss:  5551.356806695461\n",
            "\n",
            "Epoch 45/299\n",
            "----------\n",
            "Epoch Loss:  5551.248906254768\n",
            "\n",
            "Epoch 46/299\n",
            "----------\n",
            "Epoch Loss:  5551.143813788891\n",
            "\n",
            "Epoch 47/299\n",
            "----------\n",
            "Epoch Loss:  5551.041247963905\n",
            "\n",
            "Epoch 48/299\n",
            "----------\n",
            "Epoch Loss:  5550.941144287586\n",
            "\n",
            "Epoch 49/299\n",
            "----------\n",
            "Epoch Loss:  5550.843385756016\n",
            "\n",
            "Epoch 50/299\n",
            "----------\n",
            "Epoch Loss:  5550.747908294201\n",
            "\n",
            "Epoch 51/299\n",
            "----------\n",
            "Epoch Loss:  5550.65455442667\n",
            "\n",
            "Epoch 52/299\n",
            "----------\n",
            "Epoch Loss:  5550.563198924065\n",
            "\n",
            "Epoch 53/299\n",
            "----------\n",
            "Epoch Loss:  5550.473831474781\n",
            "\n",
            "Epoch 54/299\n",
            "----------\n",
            "Epoch Loss:  5550.3863760232925\n",
            "\n",
            "Epoch 55/299\n",
            "----------\n",
            "Epoch Loss:  5550.300669491291\n",
            "\n",
            "Epoch 56/299\n",
            "----------\n",
            "Epoch Loss:  5550.21669459343\n",
            "\n",
            "Epoch 57/299\n",
            "----------\n",
            "Epoch Loss:  5550.134393811226\n",
            "\n",
            "Epoch 58/299\n",
            "----------\n",
            "Epoch Loss:  5550.0536741018295\n",
            "\n",
            "Epoch 59/299\n",
            "----------\n",
            "Epoch Loss:  5549.974473655224\n",
            "\n",
            "Epoch 60/299\n",
            "----------\n",
            "Epoch Loss:  5549.896750092506\n",
            "\n",
            "Epoch 61/299\n",
            "----------\n",
            "Epoch Loss:  5549.820448100567\n",
            "\n",
            "Epoch 62/299\n",
            "----------\n",
            "Epoch Loss:  5549.745507836342\n",
            "\n",
            "Epoch 63/299\n",
            "----------\n",
            "Epoch Loss:  5549.671854138374\n",
            "\n",
            "Epoch 64/299\n",
            "----------\n",
            "Epoch Loss:  5549.599512696266\n",
            "\n",
            "Epoch 65/299\n",
            "----------\n",
            "Epoch Loss:  5549.528384566307\n",
            "\n",
            "Epoch 66/299\n",
            "----------\n",
            "Epoch Loss:  5549.458458125591\n",
            "\n",
            "Epoch 67/299\n",
            "----------\n",
            "Epoch Loss:  5549.3896916508675\n",
            "\n",
            "Epoch 68/299\n",
            "----------\n",
            "Epoch Loss:  5549.322037160397\n",
            "\n",
            "Epoch 69/299\n",
            "----------\n",
            "Epoch Loss:  5549.255436837673\n",
            "\n",
            "Epoch 70/299\n",
            "----------\n",
            "Epoch Loss:  5549.189904868603\n",
            "\n",
            "Epoch 71/299\n",
            "----------\n",
            "Epoch Loss:  5549.125378668308\n",
            "\n",
            "Epoch 72/299\n",
            "----------\n",
            "Epoch Loss:  5549.061818599701\n",
            "\n",
            "Epoch 73/299\n",
            "----------\n",
            "Epoch Loss:  5548.9992109537125\n",
            "\n",
            "Epoch 74/299\n",
            "----------\n",
            "Epoch Loss:  5548.937537193298\n",
            "\n",
            "Epoch 75/299\n",
            "----------\n",
            "Epoch Loss:  5548.876804828644\n",
            "\n",
            "Epoch 76/299\n",
            "----------\n",
            "Epoch Loss:  5548.816908419132\n",
            "\n",
            "Epoch 77/299\n",
            "----------\n",
            "Epoch Loss:  5548.757895410061\n",
            "\n",
            "Epoch 78/299\n",
            "----------\n",
            "Epoch Loss:  5548.699674069881\n",
            "\n",
            "Epoch 79/299\n",
            "----------\n",
            "Epoch Loss:  5548.642259716988\n",
            "\n",
            "Epoch 80/299\n",
            "----------\n",
            "Epoch Loss:  5548.585655212402\n",
            "\n",
            "Epoch 81/299\n",
            "----------\n",
            "Epoch Loss:  5548.529787123203\n",
            "\n",
            "Epoch 82/299\n",
            "----------\n",
            "Epoch Loss:  5548.474682927132\n",
            "\n",
            "Epoch 83/299\n",
            "----------\n",
            "Epoch Loss:  5548.42032378912\n",
            "\n",
            "Epoch 84/299\n",
            "----------\n",
            "Epoch Loss:  5548.366622745991\n",
            "\n",
            "Epoch 85/299\n",
            "----------\n",
            "Epoch Loss:  5548.313639998436\n",
            "\n",
            "Epoch 86/299\n",
            "----------\n",
            "Epoch Loss:  5548.261353731155\n",
            "\n",
            "Epoch 87/299\n",
            "----------\n",
            "Epoch Loss:  5548.209716618061\n",
            "\n",
            "Epoch 88/299\n",
            "----------\n",
            "Epoch Loss:  5548.158689916134\n",
            "\n",
            "Epoch 89/299\n",
            "----------\n",
            "Epoch Loss:  5548.108322262764\n",
            "\n",
            "Epoch 90/299\n",
            "----------\n",
            "Epoch Loss:  5548.05857616663\n",
            "\n",
            "Epoch 91/299\n",
            "----------\n",
            "Epoch Loss:  5548.009385764599\n",
            "\n",
            "Epoch 92/299\n",
            "----------\n",
            "Epoch Loss:  5547.96079236269\n",
            "\n",
            "Epoch 93/299\n",
            "----------\n",
            "Epoch Loss:  5547.912776827812\n",
            "\n",
            "Epoch 94/299\n",
            "----------\n",
            "Epoch Loss:  5547.865289926529\n",
            "\n",
            "Epoch 95/299\n",
            "----------\n",
            "Epoch Loss:  5547.818343937397\n",
            "\n",
            "Epoch 96/299\n",
            "----------\n",
            "Epoch Loss:  5547.771958291531\n",
            "\n",
            "Epoch 97/299\n",
            "----------\n",
            "Epoch Loss:  5547.726030766964\n",
            "\n",
            "Epoch 98/299\n",
            "----------\n",
            "Epoch Loss:  5547.680607378483\n",
            "\n",
            "Epoch 99/299\n",
            "----------\n",
            "Epoch Loss:  5547.635702431202\n",
            "\n",
            "Epoch 100/299\n",
            "----------\n",
            "Epoch Loss:  5547.591272950172\n",
            "\n",
            "Epoch 101/299\n",
            "----------\n",
            "Epoch Loss:  5547.5472720861435\n",
            "\n",
            "Epoch 102/299\n",
            "----------\n",
            "Epoch Loss:  5547.503715991974\n",
            "\n",
            "Epoch 103/299\n",
            "----------\n",
            "Epoch Loss:  5547.460630059242\n",
            "\n",
            "Epoch 104/299\n",
            "----------\n",
            "Epoch Loss:  5547.417933881283\n",
            "\n",
            "Epoch 105/299\n",
            "----------\n",
            "Epoch Loss:  5547.375668048859\n",
            "\n",
            "Epoch 106/299\n",
            "----------\n",
            "Epoch Loss:  5547.333781659603\n",
            "\n",
            "Epoch 107/299\n",
            "----------\n",
            "Epoch Loss:  5547.292260169983\n",
            "\n",
            "Epoch 108/299\n",
            "----------\n",
            "Epoch Loss:  5547.251104414463\n",
            "\n",
            "Epoch 109/299\n",
            "----------\n",
            "Epoch Loss:  5547.2103234529495\n",
            "\n",
            "Epoch 110/299\n",
            "----------\n",
            "Epoch Loss:  5547.169882118702\n",
            "\n",
            "Epoch 111/299\n",
            "----------\n",
            "Epoch Loss:  5547.129794061184\n",
            "\n",
            "Epoch 112/299\n",
            "----------\n",
            "Epoch Loss:  5547.090010225773\n",
            "\n",
            "Epoch 113/299\n",
            "----------\n",
            "Epoch Loss:  5547.050519526005\n",
            "\n",
            "Epoch 114/299\n",
            "----------\n",
            "Epoch Loss:  5547.011312067509\n",
            "\n",
            "Epoch 115/299\n",
            "----------\n",
            "Epoch Loss:  5546.972405612469\n",
            "\n",
            "Epoch 116/299\n",
            "----------\n",
            "Epoch Loss:  5546.933760344982\n",
            "\n",
            "Epoch 117/299\n",
            "----------\n",
            "Epoch Loss:  5546.895377337933\n",
            "\n",
            "Epoch 118/299\n",
            "----------\n",
            "Epoch Loss:  5546.857243180275\n",
            "\n",
            "Epoch 119/299\n",
            "----------\n",
            "Epoch Loss:  5546.819358348846\n",
            "\n",
            "Epoch 120/299\n",
            "----------\n",
            "Epoch Loss:  5546.781710207462\n",
            "\n",
            "Epoch 121/299\n",
            "----------\n",
            "Epoch Loss:  5546.744240999222\n",
            "\n",
            "Epoch 122/299\n",
            "----------\n",
            "Epoch Loss:  5546.706985414028\n",
            "\n",
            "Epoch 123/299\n",
            "----------\n",
            "Epoch Loss:  5546.669935107231\n",
            "\n",
            "Epoch 124/299\n",
            "----------\n",
            "Epoch Loss:  5546.633079648018\n",
            "\n",
            "Epoch 125/299\n",
            "----------\n",
            "Epoch Loss:  5546.5963988900185\n",
            "\n",
            "Epoch 126/299\n",
            "----------\n",
            "Epoch Loss:  5546.559911727905\n",
            "\n",
            "Epoch 127/299\n",
            "----------\n",
            "Epoch Loss:  5546.523552298546\n",
            "\n",
            "Epoch 128/299\n",
            "----------\n",
            "Epoch Loss:  5546.487410128117\n",
            "\n",
            "Epoch 129/299\n",
            "----------\n",
            "Epoch Loss:  5546.451386272907\n",
            "\n",
            "Epoch 130/299\n",
            "----------\n",
            "Epoch Loss:  5546.415529668331\n",
            "\n",
            "Epoch 131/299\n",
            "----------\n",
            "Epoch Loss:  5546.379871845245\n",
            "\n",
            "Epoch 132/299\n",
            "----------\n",
            "Epoch Loss:  5546.344378054142\n",
            "\n",
            "Epoch 133/299\n",
            "----------\n",
            "Epoch Loss:  5546.309019327164\n",
            "\n",
            "Epoch 134/299\n",
            "----------\n",
            "Epoch Loss:  5546.273841977119\n",
            "\n",
            "Epoch 135/299\n",
            "----------\n",
            "Epoch Loss:  5546.23883074522\n",
            "\n",
            "Epoch 136/299\n",
            "----------\n",
            "Epoch Loss:  5546.204009890556\n",
            "\n",
            "Epoch 137/299\n",
            "----------\n",
            "Epoch Loss:  5546.169396042824\n",
            "\n",
            "Epoch 138/299\n",
            "----------\n",
            "Epoch Loss:  5546.134929060936\n",
            "\n",
            "Epoch 139/299\n",
            "----------\n",
            "Epoch Loss:  5546.100655794144\n",
            "\n",
            "Epoch 140/299\n",
            "----------\n",
            "Epoch Loss:  5546.06664198637\n",
            "\n",
            "Epoch 141/299\n",
            "----------\n",
            "Epoch Loss:  5546.0328332185745\n",
            "\n",
            "Epoch 142/299\n",
            "----------\n",
            "Epoch Loss:  5545.9992299079895\n",
            "\n",
            "Epoch 143/299\n",
            "----------\n",
            "Epoch Loss:  5545.965838909149\n",
            "\n",
            "Epoch 144/299\n",
            "----------\n",
            "Epoch Loss:  5545.932761132717\n",
            "\n",
            "Epoch 145/299\n",
            "----------\n",
            "Epoch Loss:  5545.899923324585\n",
            "\n",
            "Epoch 146/299\n",
            "----------\n",
            "Epoch Loss:  5545.867347896099\n",
            "\n",
            "Epoch 147/299\n",
            "----------\n",
            "Epoch Loss:  5545.835017502308\n",
            "\n",
            "Epoch 148/299\n",
            "----------\n",
            "Epoch Loss:  5545.8030380010605\n",
            "\n",
            "Epoch 149/299\n",
            "----------\n",
            "Epoch Loss:  5545.771370470524\n",
            "\n",
            "Epoch 150/299\n",
            "----------\n",
            "Epoch Loss:  5545.7399507164955\n",
            "\n",
            "Epoch 151/299\n",
            "----------\n",
            "Epoch Loss:  5545.708857357502\n",
            "\n",
            "Epoch 152/299\n",
            "----------\n",
            "Epoch Loss:  5545.678144991398\n",
            "\n",
            "Epoch 153/299\n",
            "----------\n",
            "Epoch Loss:  5545.647738993168\n",
            "\n",
            "Epoch 154/299\n",
            "----------\n",
            "Epoch Loss:  5545.617644190788\n",
            "\n",
            "Epoch 155/299\n",
            "----------\n",
            "Epoch Loss:  5545.587903082371\n",
            "\n",
            "Epoch 156/299\n",
            "----------\n",
            "Epoch Loss:  5545.558521866798\n",
            "\n",
            "Epoch 157/299\n",
            "----------\n",
            "Epoch Loss:  5545.529453873634\n",
            "\n",
            "Epoch 158/299\n",
            "----------\n",
            "Epoch Loss:  5545.50076675415\n",
            "\n",
            "Epoch 159/299\n",
            "----------\n",
            "Epoch Loss:  5545.472451508045\n",
            "\n",
            "Epoch 160/299\n",
            "----------\n",
            "Epoch Loss:  5545.444438993931\n",
            "\n",
            "Epoch 161/299\n",
            "----------\n",
            "Epoch Loss:  5545.4168038368225\n",
            "\n",
            "Epoch 162/299\n",
            "----------\n",
            "Epoch Loss:  5545.389539361\n",
            "\n",
            "Epoch 163/299\n",
            "----------\n",
            "Epoch Loss:  5545.362588703632\n",
            "\n",
            "Epoch 164/299\n",
            "----------\n",
            "Epoch Loss:  5545.336056768894\n",
            "\n",
            "Epoch 165/299\n",
            "----------\n",
            "Epoch Loss:  5545.309764146805\n",
            "\n",
            "Epoch 166/299\n",
            "----------\n",
            "Epoch Loss:  5545.283848106861\n",
            "\n",
            "Epoch 167/299\n",
            "----------\n",
            "Epoch Loss:  5545.258257329464\n",
            "\n",
            "Epoch 168/299\n",
            "----------\n",
            "Epoch Loss:  5545.232996761799\n",
            "\n",
            "Epoch 169/299\n",
            "----------\n",
            "Epoch Loss:  5545.208078861237\n",
            "\n",
            "Epoch 170/299\n",
            "----------\n",
            "Epoch Loss:  5545.18347966671\n",
            "\n",
            "Epoch 171/299\n",
            "----------\n",
            "Epoch Loss:  5545.15917468071\n",
            "\n",
            "Epoch 172/299\n",
            "----------\n",
            "Epoch Loss:  5545.135185182095\n",
            "\n",
            "Epoch 173/299\n",
            "----------\n",
            "Epoch Loss:  5545.111492455006\n",
            "\n",
            "Epoch 174/299\n",
            "----------\n",
            "Epoch Loss:  5545.088112235069\n",
            "\n",
            "Epoch 175/299\n",
            "----------\n",
            "Epoch Loss:  5545.0649726986885\n",
            "\n",
            "Epoch 176/299\n",
            "----------\n",
            "Epoch Loss:  5545.042178988457\n",
            "\n",
            "Epoch 177/299\n",
            "----------\n",
            "Epoch Loss:  5545.019623547792\n",
            "\n",
            "Epoch 178/299\n",
            "----------\n",
            "Epoch Loss:  5544.997359752655\n",
            "\n",
            "Epoch 179/299\n",
            "----------\n",
            "Epoch Loss:  5544.975325226784\n",
            "\n",
            "Epoch 180/299\n",
            "----------\n",
            "Epoch Loss:  5544.953592687845\n",
            "\n",
            "Epoch 181/299\n",
            "----------\n",
            "Epoch Loss:  5544.932070314884\n",
            "\n",
            "Epoch 182/299\n",
            "----------\n",
            "Epoch Loss:  5544.910851478577\n",
            "\n",
            "Epoch 183/299\n",
            "----------\n",
            "Epoch Loss:  5544.889778614044\n",
            "\n",
            "Epoch 184/299\n",
            "----------\n",
            "Epoch Loss:  5544.869000405073\n",
            "\n",
            "Epoch 185/299\n",
            "----------\n",
            "Epoch Loss:  5544.848445594311\n",
            "\n",
            "Epoch 186/299\n",
            "----------\n",
            "Epoch Loss:  5544.828123599291\n",
            "\n",
            "Epoch 187/299\n",
            "----------\n",
            "Epoch Loss:  5544.807997316122\n",
            "\n",
            "Epoch 188/299\n",
            "----------\n",
            "Epoch Loss:  5544.788079380989\n",
            "\n",
            "Epoch 189/299\n",
            "----------\n",
            "Epoch Loss:  5544.768404990435\n",
            "\n",
            "Epoch 190/299\n",
            "----------\n",
            "Epoch Loss:  5544.7488869428635\n",
            "\n",
            "Epoch 191/299\n",
            "----------\n",
            "Epoch Loss:  5544.729562401772\n",
            "\n",
            "Epoch 192/299\n",
            "----------\n",
            "Epoch Loss:  5544.7104568481445\n",
            "\n",
            "Epoch 193/299\n",
            "----------\n",
            "Epoch Loss:  5544.691549748182\n",
            "\n",
            "Epoch 194/299\n",
            "----------\n",
            "Epoch Loss:  5544.672818034887\n",
            "\n",
            "Epoch 195/299\n",
            "----------\n",
            "Epoch Loss:  5544.654222607613\n",
            "\n",
            "Epoch 196/299\n",
            "----------\n",
            "Epoch Loss:  5544.635882258415\n",
            "\n",
            "Epoch 197/299\n",
            "----------\n",
            "Epoch Loss:  5544.617671340704\n",
            "\n",
            "Epoch 198/299\n",
            "----------\n",
            "Epoch Loss:  5544.599624812603\n",
            "\n",
            "Epoch 199/299\n",
            "----------\n",
            "Epoch Loss:  5544.5817766189575\n",
            "\n",
            "Epoch 200/299\n",
            "----------\n",
            "Epoch Loss:  5544.564036309719\n",
            "\n",
            "Epoch 201/299\n",
            "----------\n",
            "Epoch Loss:  5544.546480089426\n",
            "\n",
            "Epoch 202/299\n",
            "----------\n",
            "Epoch Loss:  5544.529050946236\n",
            "\n",
            "Epoch 203/299\n",
            "----------\n",
            "Epoch Loss:  5544.511796236038\n",
            "\n",
            "Epoch 204/299\n",
            "----------\n",
            "Epoch Loss:  5544.494694232941\n",
            "\n",
            "Epoch 205/299\n",
            "----------\n",
            "Epoch Loss:  5544.477671086788\n",
            "\n",
            "Epoch 206/299\n",
            "----------\n",
            "Epoch Loss:  5544.460866570473\n",
            "\n",
            "Epoch 207/299\n",
            "----------\n",
            "Epoch Loss:  5544.444202005863\n",
            "\n",
            "Epoch 208/299\n",
            "----------\n",
            "Epoch Loss:  5544.427636563778\n",
            "\n",
            "Epoch 209/299\n",
            "----------\n",
            "Epoch Loss:  5544.411188960075\n",
            "\n",
            "Epoch 210/299\n",
            "----------\n",
            "Epoch Loss:  5544.394912391901\n",
            "\n",
            "Epoch 211/299\n",
            "----------\n",
            "Epoch Loss:  5544.378772765398\n",
            "\n",
            "Epoch 212/299\n",
            "----------\n",
            "Epoch Loss:  5544.3627207279205\n",
            "\n",
            "Epoch 213/299\n",
            "----------\n",
            "Epoch Loss:  5544.346738040447\n",
            "\n",
            "Epoch 214/299\n",
            "----------\n",
            "Epoch Loss:  5544.330912411213\n",
            "\n",
            "Epoch 215/299\n",
            "----------\n",
            "Epoch Loss:  5544.3152668476105\n",
            "\n",
            "Epoch 216/299\n",
            "----------\n",
            "Epoch Loss:  5544.2997516691685\n",
            "\n",
            "Epoch 217/299\n",
            "----------\n",
            "Epoch Loss:  5544.284234791994\n",
            "\n",
            "Epoch 218/299\n",
            "----------\n",
            "Epoch Loss:  5544.268885105848\n",
            "\n",
            "Epoch 219/299\n",
            "----------\n",
            "Epoch Loss:  5544.2536298930645\n",
            "\n",
            "Epoch 220/299\n",
            "----------\n",
            "Epoch Loss:  5544.238568454981\n",
            "\n",
            "Epoch 221/299\n",
            "----------\n",
            "Epoch Loss:  5544.223567724228\n",
            "\n",
            "Epoch 222/299\n",
            "----------\n",
            "Epoch Loss:  5544.208613097668\n",
            "\n",
            "Epoch 223/299\n",
            "----------\n",
            "Epoch Loss:  5544.193877667189\n",
            "\n",
            "Epoch 224/299\n",
            "----------\n",
            "Epoch Loss:  5544.179178416729\n",
            "\n",
            "Epoch 225/299\n",
            "----------\n",
            "Epoch Loss:  5544.164554864168\n",
            "\n",
            "Epoch 226/299\n",
            "----------\n",
            "Epoch Loss:  5544.149989157915\n",
            "\n",
            "Epoch 227/299\n",
            "----------\n",
            "Epoch Loss:  5544.13563016057\n",
            "\n",
            "Epoch 228/299\n",
            "----------\n",
            "Epoch Loss:  5544.121289968491\n",
            "\n",
            "Epoch 229/299\n",
            "----------\n",
            "Epoch Loss:  5544.107036143541\n",
            "\n",
            "Epoch 230/299\n",
            "----------\n",
            "Epoch Loss:  5544.092909276485\n",
            "\n",
            "Epoch 231/299\n",
            "----------\n",
            "Epoch Loss:  5544.078839421272\n",
            "\n",
            "Epoch 232/299\n",
            "----------\n",
            "Epoch Loss:  5544.064903765917\n",
            "\n",
            "Epoch 233/299\n",
            "----------\n",
            "Epoch Loss:  5544.051076591015\n",
            "\n",
            "Epoch 234/299\n",
            "----------\n",
            "Epoch Loss:  5544.037320137024\n",
            "\n",
            "Epoch 235/299\n",
            "----------\n",
            "Epoch Loss:  5544.023567259312\n",
            "\n",
            "Epoch 236/299\n",
            "----------\n",
            "Epoch Loss:  5544.009998828173\n",
            "\n",
            "Epoch 237/299\n",
            "----------\n",
            "Epoch Loss:  5543.996460199356\n",
            "\n",
            "Epoch 238/299\n",
            "----------\n",
            "Epoch Loss:  5543.983018070459\n",
            "\n",
            "Epoch 239/299\n",
            "----------\n",
            "Epoch Loss:  5543.969616502523\n",
            "\n",
            "Epoch 240/299\n",
            "----------\n",
            "Epoch Loss:  5543.956348180771\n",
            "\n",
            "Epoch 241/299\n",
            "----------\n",
            "Epoch Loss:  5543.943146377802\n",
            "\n",
            "Epoch 242/299\n",
            "----------\n",
            "Epoch Loss:  5543.92995634675\n",
            "\n",
            "Epoch 243/299\n",
            "----------\n",
            "Epoch Loss:  5543.9169217944145\n",
            "\n",
            "Epoch 244/299\n",
            "----------\n",
            "Epoch Loss:  5543.903995424509\n",
            "\n",
            "Epoch 245/299\n",
            "----------\n",
            "Epoch Loss:  5543.8910131156445\n",
            "\n",
            "Epoch 246/299\n",
            "----------\n",
            "Epoch Loss:  5543.878198891878\n",
            "\n",
            "Epoch 247/299\n",
            "----------\n",
            "Epoch Loss:  5543.865414351225\n",
            "\n",
            "Epoch 248/299\n",
            "----------\n",
            "Epoch Loss:  5543.852724164724\n",
            "\n",
            "Epoch 249/299\n",
            "----------\n",
            "Epoch Loss:  5543.840122818947\n",
            "\n",
            "Epoch 250/299\n",
            "----------\n",
            "Epoch Loss:  5543.827582657337\n",
            "\n",
            "Epoch 251/299\n",
            "----------\n",
            "Epoch Loss:  5543.815058082342\n",
            "\n",
            "Epoch 252/299\n",
            "----------\n",
            "Epoch Loss:  5543.802620798349\n",
            "\n",
            "Epoch 253/299\n",
            "----------\n",
            "Epoch Loss:  5543.790248006582\n",
            "\n",
            "Epoch 254/299\n",
            "----------\n",
            "Epoch Loss:  5543.777861386538\n",
            "\n",
            "Epoch 255/299\n",
            "----------\n",
            "Epoch Loss:  5543.765677303076\n",
            "\n",
            "Epoch 256/299\n",
            "----------\n",
            "Epoch Loss:  5543.7535001933575\n",
            "\n",
            "Epoch 257/299\n",
            "----------\n",
            "Epoch Loss:  5543.741370290518\n",
            "\n",
            "Epoch 258/299\n",
            "----------\n",
            "Epoch Loss:  5543.7293602228165\n",
            "\n",
            "Epoch 259/299\n",
            "----------\n",
            "Epoch Loss:  5543.717331618071\n",
            "\n",
            "Epoch 260/299\n",
            "----------\n",
            "Epoch Loss:  5543.70539650321\n",
            "\n",
            "Epoch 261/299\n",
            "----------\n",
            "Epoch Loss:  5543.693529784679\n",
            "\n",
            "Epoch 262/299\n",
            "----------\n",
            "Epoch Loss:  5543.68171620369\n",
            "\n",
            "Epoch 263/299\n",
            "----------\n",
            "Epoch Loss:  5543.669921576977\n",
            "\n",
            "Epoch 264/299\n",
            "----------\n",
            "Epoch Loss:  5543.65823379159\n",
            "\n",
            "Epoch 265/299\n",
            "----------\n",
            "Epoch Loss:  5543.64658883214\n",
            "\n",
            "Epoch 266/299\n",
            "----------\n",
            "Epoch Loss:  5543.635036230087\n",
            "\n",
            "Epoch 267/299\n",
            "----------\n",
            "Epoch Loss:  5543.623489528894\n",
            "\n",
            "Epoch 268/299\n",
            "----------\n",
            "Epoch Loss:  5543.611965566874\n",
            "\n",
            "Epoch 269/299\n",
            "----------\n",
            "Epoch Loss:  5543.600524663925\n",
            "\n",
            "Epoch 270/299\n",
            "----------\n",
            "Epoch Loss:  5543.589123189449\n",
            "\n",
            "Epoch 271/299\n",
            "----------\n",
            "Epoch Loss:  5543.577778607607\n",
            "\n",
            "Epoch 272/299\n",
            "----------\n",
            "Epoch Loss:  5543.566516846418\n",
            "\n",
            "Epoch 273/299\n",
            "----------\n",
            "Epoch Loss:  5543.5552514493465\n",
            "\n",
            "Epoch 274/299\n",
            "----------\n",
            "Epoch Loss:  5543.544107079506\n",
            "\n",
            "Epoch 275/299\n",
            "----------\n",
            "Epoch Loss:  5543.532895714045\n",
            "\n",
            "Epoch 276/299\n",
            "----------\n",
            "Epoch Loss:  5543.521863102913\n",
            "\n",
            "Epoch 277/299\n",
            "----------\n",
            "Epoch Loss:  5543.510863900185\n",
            "\n",
            "Epoch 278/299\n",
            "----------\n",
            "Epoch Loss:  5543.499822944403\n",
            "\n",
            "Epoch 279/299\n",
            "----------\n",
            "Epoch Loss:  5543.488878309727\n",
            "\n",
            "Epoch 280/299\n",
            "----------\n",
            "Epoch Loss:  5543.477922677994\n",
            "\n",
            "Epoch 281/299\n",
            "----------\n",
            "Epoch Loss:  5543.467048794031\n",
            "\n",
            "Epoch 282/299\n",
            "----------\n",
            "Epoch Loss:  5543.456252992153\n",
            "\n",
            "Epoch 283/299\n",
            "----------\n",
            "Epoch Loss:  5543.445485383272\n",
            "\n",
            "Epoch 284/299\n",
            "----------\n",
            "Epoch Loss:  5543.434772580862\n",
            "\n",
            "Epoch 285/299\n",
            "----------\n",
            "Epoch Loss:  5543.4240780472755\n",
            "\n",
            "Epoch 286/299\n",
            "----------\n",
            "Epoch Loss:  5543.41335773468\n",
            "\n",
            "Epoch 287/299\n",
            "----------\n",
            "Epoch Loss:  5543.402727603912\n",
            "\n",
            "Epoch 288/299\n",
            "----------\n",
            "Epoch Loss:  5543.392208009958\n",
            "\n",
            "Epoch 289/299\n",
            "----------\n",
            "Epoch Loss:  5543.381675124168\n",
            "\n",
            "Epoch 290/299\n",
            "----------\n",
            "Epoch Loss:  5543.37120655179\n",
            "\n",
            "Epoch 291/299\n",
            "----------\n",
            "Epoch Loss:  5543.360672533512\n",
            "\n",
            "Epoch 292/299\n",
            "----------\n",
            "Epoch Loss:  5543.35025459528\n",
            "\n",
            "Epoch 293/299\n",
            "----------\n",
            "Epoch Loss:  5543.33993858099\n",
            "\n",
            "Epoch 294/299\n",
            "----------\n",
            "Epoch Loss:  5543.329552769661\n",
            "\n",
            "Epoch 295/299\n",
            "----------\n",
            "Epoch Loss:  5543.319198638201\n",
            "\n",
            "Epoch 296/299\n",
            "----------\n",
            "Epoch Loss:  5543.308981448412\n",
            "\n",
            "Epoch 297/299\n",
            "----------\n",
            "Epoch Loss:  5543.2987449765205\n",
            "\n",
            "Epoch 298/299\n",
            "----------\n",
            "Epoch Loss:  5543.288570791483\n",
            "\n",
            "Epoch 299/299\n",
            "----------\n",
            "Epoch Loss:  5543.278393357992\n",
            "\n",
            "Training complete in 112m 32s\n",
            "End\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRkBUrSvH93L"
      },
      "source": [
        "# Testing model 2 on all three test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M95K-VWD95Zx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9803c243-bc43-4587-ec90-889626b4322b"
      },
      "source": [
        "model = coherenceModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "coherenceModel(\n",
              "  (conv2DSent1): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent2): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent3): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (avgPool2DSent1): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent2): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent3): AdaptiveAvgPool1d(output_size=1)\n",
              "  (hiddenLayer): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
              "  (finalLayer): AvgPool1d(kernel_size=(48,), stride=(48,), padding=(0,))\n",
              "  (endSigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z04eiLTxGYKs",
        "outputId": "e9c21ee8-a036-4bab-f505-4fa8f48f7024"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/combined/combined_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  combined_test_embeddings = pickle.load(fp)\n",
        "  \n",
        "coherentScores = test_model(model, \"coherenceModelCombined\",0, combined_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelCombined\",1, combined_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  77.07142857142857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF0DbgKQquJi",
        "outputId": "b82061b6-566b-4331-efc5-04368f107abb"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/separate/separate_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  separate_test_embeddings = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelCombined\",0, separate_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelCombined\",1, separate_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  74.53571428571429\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6fjLoNyGa9x",
        "outputId": "3dd58ad2-a92a-4048-9e48-ee8bea3d654c"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/accident/datasetNTSBTesting.txt\", \"rb\") as fp:   # Unpickling\n",
        "  datasetNTSBTesting = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelCombined\",0, datasetNTSBTesting)\n",
        "incoherentScores = test_model(model, \"coherenceModelCombined\",1, datasetNTSBTesting)\n",
        "\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  69.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMHGv_xH0A1"
      },
      "source": [
        "# Training dataset3: Accident reports dataset which is a standard dataset for text coherence tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E9Hoy_ZEip4"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/accident/datasetNTSBTraining.txt\", \"rb\") as fp:   # Unpickling\n",
        "  datasetNTSBTraining = pickle.load(fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vpmjsg7ihEak",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a361847d-5b01-44ad-f8e0-5292cf5b1854"
      },
      "source": [
        "model = coherenceModel()\n",
        "_lr = 2e-3\n",
        "optimizer = Adam(model.parameters(), lr=_lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Start\")\n",
        "model_ft = train_model(model, \"coherenceModelAccident\", optimizer, datasetNTSBTraining, num_epochs=300)\n",
        "print(\"End\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start\n",
            "Start Training\n",
            "Epoch 0/299\n",
            "----------\n",
            "Epoch Loss:  1353.2238913476467\n",
            "\n",
            "Epoch 1/299\n",
            "----------\n",
            "Epoch Loss:  1359.3757936060429\n",
            "\n",
            "Epoch 2/299\n",
            "----------\n",
            "Epoch Loss:  1354.6053730845451\n",
            "\n",
            "Epoch 3/299\n",
            "----------\n",
            "Epoch Loss:  1351.1912798285484\n",
            "\n",
            "Epoch 4/299\n",
            "----------\n",
            "Epoch Loss:  1348.2134356796741\n",
            "\n",
            "Epoch 5/299\n",
            "----------\n",
            "Epoch Loss:  1345.5197977870703\n",
            "\n",
            "Epoch 6/299\n",
            "----------\n",
            "Epoch Loss:  1343.112528219819\n",
            "\n",
            "Epoch 7/299\n",
            "----------\n",
            "Epoch Loss:  1340.989823386073\n",
            "\n",
            "Epoch 8/299\n",
            "----------\n",
            "Epoch Loss:  1339.1265727132559\n",
            "\n",
            "Epoch 9/299\n",
            "----------\n",
            "Epoch Loss:  1337.4860010445118\n",
            "\n",
            "Epoch 10/299\n",
            "----------\n",
            "Epoch Loss:  1336.030847594142\n",
            "\n",
            "Epoch 11/299\n",
            "----------\n",
            "Epoch Loss:  1334.729093492031\n",
            "\n",
            "Epoch 12/299\n",
            "----------\n",
            "Epoch Loss:  1333.5550366342068\n",
            "\n",
            "Epoch 13/299\n",
            "----------\n",
            "Epoch Loss:  1332.488572359085\n",
            "\n",
            "Epoch 14/299\n",
            "----------\n",
            "Epoch Loss:  1331.5140054523945\n",
            "\n",
            "Epoch 15/299\n",
            "----------\n",
            "Epoch Loss:  1330.6189009323716\n",
            "\n",
            "Epoch 16/299\n",
            "----------\n",
            "Epoch Loss:  1329.7931881174445\n",
            "\n",
            "Epoch 17/299\n",
            "----------\n",
            "Epoch Loss:  1329.0285266116261\n",
            "\n",
            "Epoch 18/299\n",
            "----------\n",
            "Epoch Loss:  1328.3179090991616\n",
            "\n",
            "Epoch 19/299\n",
            "----------\n",
            "Epoch Loss:  1327.655262567103\n",
            "\n",
            "Epoch 20/299\n",
            "----------\n",
            "Epoch Loss:  1327.0355194583535\n",
            "\n",
            "Epoch 21/299\n",
            "----------\n",
            "Epoch Loss:  1326.4540710672736\n",
            "\n",
            "Epoch 22/299\n",
            "----------\n",
            "Epoch Loss:  1325.9070586413145\n",
            "\n",
            "Epoch 23/299\n",
            "----------\n",
            "Epoch Loss:  1325.3910042494535\n",
            "\n",
            "Epoch 24/299\n",
            "----------\n",
            "Epoch Loss:  1324.902834303677\n",
            "\n",
            "Epoch 25/299\n",
            "----------\n",
            "Epoch Loss:  1324.4399235248566\n",
            "\n",
            "Epoch 26/299\n",
            "----------\n",
            "Epoch Loss:  1323.9998686090112\n",
            "\n",
            "Epoch 27/299\n",
            "----------\n",
            "Epoch Loss:  1323.5806411504745\n",
            "\n",
            "Epoch 28/299\n",
            "----------\n",
            "Epoch Loss:  1323.180340230465\n",
            "\n",
            "Epoch 29/299\n",
            "----------\n",
            "Epoch Loss:  1322.7973338216543\n",
            "\n",
            "Epoch 30/299\n",
            "----------\n",
            "Epoch Loss:  1322.4301808401942\n",
            "\n",
            "Epoch 31/299\n",
            "----------\n",
            "Epoch Loss:  1322.0775722116232\n",
            "\n",
            "Epoch 32/299\n",
            "----------\n",
            "Epoch Loss:  1321.7383413687348\n",
            "\n",
            "Epoch 33/299\n",
            "----------\n",
            "Epoch Loss:  1321.4114516079426\n",
            "\n",
            "Epoch 34/299\n",
            "----------\n",
            "Epoch Loss:  1321.0959929972887\n",
            "\n",
            "Epoch 35/299\n",
            "----------\n",
            "Epoch Loss:  1320.7911110296845\n",
            "\n",
            "Epoch 36/299\n",
            "----------\n",
            "Epoch Loss:  1320.4960474818945\n",
            "\n",
            "Epoch 37/299\n",
            "----------\n",
            "Epoch Loss:  1320.2101304531097\n",
            "\n",
            "Epoch 38/299\n",
            "----------\n",
            "Epoch Loss:  1319.9327429905534\n",
            "\n",
            "Epoch 39/299\n",
            "----------\n",
            "Epoch Loss:  1319.6633196175098\n",
            "\n",
            "Epoch 40/299\n",
            "----------\n",
            "Epoch Loss:  1319.401370063424\n",
            "\n",
            "Epoch 41/299\n",
            "----------\n",
            "Epoch Loss:  1319.1464142762125\n",
            "\n",
            "Epoch 42/299\n",
            "----------\n",
            "Epoch Loss:  1318.898050982505\n",
            "\n",
            "Epoch 43/299\n",
            "----------\n",
            "Epoch Loss:  1318.6558708399534\n",
            "\n",
            "Epoch 44/299\n",
            "----------\n",
            "Epoch Loss:  1318.4195552878082\n",
            "\n",
            "Epoch 45/299\n",
            "----------\n",
            "Epoch Loss:  1318.1887373067439\n",
            "\n",
            "Epoch 46/299\n",
            "----------\n",
            "Epoch Loss:  1317.963159237057\n",
            "\n",
            "Epoch 47/299\n",
            "----------\n",
            "Epoch Loss:  1317.7425171025097\n",
            "\n",
            "Epoch 48/299\n",
            "----------\n",
            "Epoch Loss:  1317.5265777893364\n",
            "\n",
            "Epoch 49/299\n",
            "----------\n",
            "Epoch Loss:  1317.3150818757713\n",
            "\n",
            "Epoch 50/299\n",
            "----------\n",
            "Epoch Loss:  1317.1078463084996\n",
            "\n",
            "Epoch 51/299\n",
            "----------\n",
            "Epoch Loss:  1316.904637672007\n",
            "\n",
            "Epoch 52/299\n",
            "----------\n",
            "Epoch Loss:  1316.7052931897342\n",
            "\n",
            "Epoch 53/299\n",
            "----------\n",
            "Epoch Loss:  1316.5096348114312\n",
            "\n",
            "Epoch 54/299\n",
            "----------\n",
            "Epoch Loss:  1316.3175072744489\n",
            "\n",
            "Epoch 55/299\n",
            "----------\n",
            "Epoch Loss:  1316.1287269368768\n",
            "\n",
            "Epoch 56/299\n",
            "----------\n",
            "Epoch Loss:  1315.9431868679821\n",
            "\n",
            "Epoch 57/299\n",
            "----------\n",
            "Epoch Loss:  1315.7607251368463\n",
            "\n",
            "Epoch 58/299\n",
            "----------\n",
            "Epoch Loss:  1315.5812443457544\n",
            "\n",
            "Epoch 59/299\n",
            "----------\n",
            "Epoch Loss:  1315.404618062079\n",
            "\n",
            "Epoch 60/299\n",
            "----------\n",
            "Epoch Loss:  1315.2307399101555\n",
            "\n",
            "Epoch 61/299\n",
            "----------\n",
            "Epoch Loss:  1315.0595100335777\n",
            "\n",
            "Epoch 62/299\n",
            "----------\n",
            "Epoch Loss:  1314.8908253349364\n",
            "\n",
            "Epoch 63/299\n",
            "----------\n",
            "Epoch Loss:  1314.7245720364153\n",
            "\n",
            "Epoch 64/299\n",
            "----------\n",
            "Epoch Loss:  1314.5606738515198\n",
            "\n",
            "Epoch 65/299\n",
            "----------\n",
            "Epoch Loss:  1314.3991257995367\n",
            "\n",
            "Epoch 66/299\n",
            "----------\n",
            "Epoch Loss:  1314.2396912388504\n",
            "\n",
            "Epoch 67/299\n",
            "----------\n",
            "Epoch Loss:  1314.0824422985315\n",
            "\n",
            "Epoch 68/299\n",
            "----------\n",
            "Epoch Loss:  1313.9272613190114\n",
            "\n",
            "Epoch 69/299\n",
            "----------\n",
            "Epoch Loss:  1313.774075794965\n",
            "\n",
            "Epoch 70/299\n",
            "----------\n",
            "Epoch Loss:  1313.6228332556784\n",
            "\n",
            "Epoch 71/299\n",
            "----------\n",
            "Epoch Loss:  1313.473461586982\n",
            "\n",
            "Epoch 72/299\n",
            "----------\n",
            "Epoch Loss:  1313.3258848190308\n",
            "\n",
            "Epoch 73/299\n",
            "----------\n",
            "Epoch Loss:  1313.1800910420716\n",
            "\n",
            "Epoch 74/299\n",
            "----------\n",
            "Epoch Loss:  1313.0360211469233\n",
            "\n",
            "Epoch 75/299\n",
            "----------\n",
            "Epoch Loss:  1312.8935842327774\n",
            "\n",
            "Epoch 76/299\n",
            "----------\n",
            "Epoch Loss:  1312.752791043371\n",
            "\n",
            "Epoch 77/299\n",
            "----------\n",
            "Epoch Loss:  1312.6135436035693\n",
            "\n",
            "Epoch 78/299\n",
            "----------\n",
            "Epoch Loss:  1312.4758528321981\n",
            "\n",
            "Epoch 79/299\n",
            "----------\n",
            "Epoch Loss:  1312.3396447598934\n",
            "\n",
            "Epoch 80/299\n",
            "----------\n",
            "Epoch Loss:  1312.2048831917346\n",
            "\n",
            "Epoch 81/299\n",
            "----------\n",
            "Epoch Loss:  1312.0715254321694\n",
            "\n",
            "Epoch 82/299\n",
            "----------\n",
            "Epoch Loss:  1311.9395346418023\n",
            "\n",
            "Epoch 83/299\n",
            "----------\n",
            "Epoch Loss:  1311.808909472078\n",
            "\n",
            "Epoch 84/299\n",
            "----------\n",
            "Epoch Loss:  1311.6795612089336\n",
            "\n",
            "Epoch 85/299\n",
            "----------\n",
            "Epoch Loss:  1311.551517367363\n",
            "\n",
            "Epoch 86/299\n",
            "----------\n",
            "Epoch Loss:  1311.4246757775545\n",
            "\n",
            "Epoch 87/299\n",
            "----------\n",
            "Epoch Loss:  1311.2990718968213\n",
            "\n",
            "Epoch 88/299\n",
            "----------\n",
            "Epoch Loss:  1311.1746399216354\n",
            "\n",
            "Epoch 89/299\n",
            "----------\n",
            "Epoch Loss:  1311.0513653606176\n",
            "\n",
            "Epoch 90/299\n",
            "----------\n",
            "Epoch Loss:  1310.929216567427\n",
            "\n",
            "Epoch 91/299\n",
            "----------\n",
            "Epoch Loss:  1310.8081768974662\n",
            "\n",
            "Epoch 92/299\n",
            "----------\n",
            "Epoch Loss:  1310.6882075034082\n",
            "\n",
            "Epoch 93/299\n",
            "----------\n",
            "Epoch Loss:  1310.5692790076137\n",
            "\n",
            "Epoch 94/299\n",
            "----------\n",
            "Epoch Loss:  1310.4514266252518\n",
            "\n",
            "Epoch 95/299\n",
            "----------\n",
            "Epoch Loss:  1310.3345333337784\n",
            "\n",
            "Epoch 96/299\n",
            "----------\n",
            "Epoch Loss:  1310.2186577245593\n",
            "\n",
            "Epoch 97/299\n",
            "----------\n",
            "Epoch Loss:  1310.1037056669593\n",
            "\n",
            "Epoch 98/299\n",
            "----------\n",
            "Epoch Loss:  1309.989750429988\n",
            "\n",
            "Epoch 99/299\n",
            "----------\n",
            "Epoch Loss:  1309.8766745589674\n",
            "\n",
            "Epoch 100/299\n",
            "----------\n",
            "Epoch Loss:  1309.764527015388\n",
            "\n",
            "Epoch 101/299\n",
            "----------\n",
            "Epoch Loss:  1309.6532659120858\n",
            "\n",
            "Epoch 102/299\n",
            "----------\n",
            "Epoch Loss:  1309.5428922884166\n",
            "\n",
            "Epoch 103/299\n",
            "----------\n",
            "Epoch Loss:  1309.4333478622139\n",
            "\n",
            "Epoch 104/299\n",
            "----------\n",
            "Epoch Loss:  1309.3246258310974\n",
            "\n",
            "Epoch 105/299\n",
            "----------\n",
            "Epoch Loss:  1309.2167278304696\n",
            "\n",
            "Epoch 106/299\n",
            "----------\n",
            "Epoch Loss:  1309.1096727736294\n",
            "\n",
            "Epoch 107/299\n",
            "----------\n",
            "Epoch Loss:  1309.0033590607345\n",
            "\n",
            "Epoch 108/299\n",
            "----------\n",
            "Epoch Loss:  1308.8978388048708\n",
            "\n",
            "Epoch 109/299\n",
            "----------\n",
            "Epoch Loss:  1308.7930765114725\n",
            "\n",
            "Epoch 110/299\n",
            "----------\n",
            "Epoch Loss:  1308.6890212707222\n",
            "\n",
            "Epoch 111/299\n",
            "----------\n",
            "Epoch Loss:  1308.585742238909\n",
            "\n",
            "Epoch 112/299\n",
            "----------\n",
            "Epoch Loss:  1308.4831408821046\n",
            "\n",
            "Epoch 113/299\n",
            "----------\n",
            "Epoch Loss:  1308.3812648914754\n",
            "\n",
            "Epoch 114/299\n",
            "----------\n",
            "Epoch Loss:  1308.2800812721252\n",
            "\n",
            "Epoch 115/299\n",
            "----------\n",
            "Epoch Loss:  1308.1795608326793\n",
            "\n",
            "Epoch 116/299\n",
            "----------\n",
            "Epoch Loss:  1308.0797365605831\n",
            "\n",
            "Epoch 117/299\n",
            "----------\n",
            "Epoch Loss:  1307.980512805283\n",
            "\n",
            "Epoch 118/299\n",
            "----------\n",
            "Epoch Loss:  1307.8819874152541\n",
            "\n",
            "Epoch 119/299\n",
            "----------\n",
            "Epoch Loss:  1307.7840534336865\n",
            "\n",
            "Epoch 120/299\n",
            "----------\n",
            "Epoch Loss:  1307.6867768540978\n",
            "\n",
            "Epoch 121/299\n",
            "----------\n",
            "Epoch Loss:  1307.5900796465576\n",
            "\n",
            "Epoch 122/299\n",
            "----------\n",
            "Epoch Loss:  1307.4939903840423\n",
            "\n",
            "Epoch 123/299\n",
            "----------\n",
            "Epoch Loss:  1307.398502893746\n",
            "\n",
            "Epoch 124/299\n",
            "----------\n",
            "Epoch Loss:  1307.303577505052\n",
            "\n",
            "Epoch 125/299\n",
            "----------\n",
            "Epoch Loss:  1307.2092180401087\n",
            "\n",
            "Epoch 126/299\n",
            "----------\n",
            "Epoch Loss:  1307.1154398694634\n",
            "\n",
            "Epoch 127/299\n",
            "----------\n",
            "Epoch Loss:  1307.022201243788\n",
            "\n",
            "Epoch 128/299\n",
            "----------\n",
            "Epoch Loss:  1306.9295031875372\n",
            "\n",
            "Epoch 129/299\n",
            "----------\n",
            "Epoch Loss:  1306.8373488672078\n",
            "\n",
            "Epoch 130/299\n",
            "----------\n",
            "Epoch Loss:  1306.745718292892\n",
            "\n",
            "Epoch 131/299\n",
            "----------\n",
            "Epoch Loss:  1306.654586315155\n",
            "\n",
            "Epoch 132/299\n",
            "----------\n",
            "Epoch Loss:  1306.564003419131\n",
            "\n",
            "Epoch 133/299\n",
            "----------\n",
            "Epoch Loss:  1306.4738917909563\n",
            "\n",
            "Epoch 134/299\n",
            "----------\n",
            "Epoch Loss:  1306.3842751458287\n",
            "\n",
            "Epoch 135/299\n",
            "----------\n",
            "Epoch Loss:  1306.2951538227499\n",
            "\n",
            "Epoch 136/299\n",
            "----------\n",
            "Epoch Loss:  1306.2065181471407\n",
            "\n",
            "Epoch 137/299\n",
            "----------\n",
            "Epoch Loss:  1306.1183359995484\n",
            "\n",
            "Epoch 138/299\n",
            "----------\n",
            "Epoch Loss:  1306.0306454524398\n",
            "\n",
            "Epoch 139/299\n",
            "----------\n",
            "Epoch Loss:  1305.943395793438\n",
            "\n",
            "Epoch 140/299\n",
            "----------\n",
            "Epoch Loss:  1305.8565921746194\n",
            "\n",
            "Epoch 141/299\n",
            "----------\n",
            "Epoch Loss:  1305.7702283989638\n",
            "\n",
            "Epoch 142/299\n",
            "----------\n",
            "Epoch Loss:  1305.6843423098326\n",
            "\n",
            "Epoch 143/299\n",
            "----------\n",
            "Epoch Loss:  1305.5988410729915\n",
            "\n",
            "Epoch 144/299\n",
            "----------\n",
            "Epoch Loss:  1305.5138002503663\n",
            "\n",
            "Epoch 145/299\n",
            "----------\n",
            "Epoch Loss:  1305.4291640110314\n",
            "\n",
            "Epoch 146/299\n",
            "----------\n",
            "Epoch Loss:  1305.3449382279068\n",
            "\n",
            "Epoch 147/299\n",
            "----------\n",
            "Epoch Loss:  1305.2611121144146\n",
            "\n",
            "Epoch 148/299\n",
            "----------\n",
            "Epoch Loss:  1305.177699699998\n",
            "\n",
            "Epoch 149/299\n",
            "----------\n",
            "Epoch Loss:  1305.0947112943977\n",
            "\n",
            "Epoch 150/299\n",
            "----------\n",
            "Epoch Loss:  1305.012074496597\n",
            "\n",
            "Epoch 151/299\n",
            "----------\n",
            "Epoch Loss:  1304.9298372007906\n",
            "\n",
            "Epoch 152/299\n",
            "----------\n",
            "Epoch Loss:  1304.8479875084013\n",
            "\n",
            "Epoch 153/299\n",
            "----------\n",
            "Epoch Loss:  1304.766508532688\n",
            "\n",
            "Epoch 154/299\n",
            "----------\n",
            "Epoch Loss:  1304.685391800478\n",
            "\n",
            "Epoch 155/299\n",
            "----------\n",
            "Epoch Loss:  1304.604643760249\n",
            "\n",
            "Epoch 156/299\n",
            "----------\n",
            "Epoch Loss:  1304.5243043210357\n",
            "\n",
            "Epoch 157/299\n",
            "----------\n",
            "Epoch Loss:  1304.4442525915802\n",
            "\n",
            "Epoch 158/299\n",
            "----------\n",
            "Epoch Loss:  1304.3645761776716\n",
            "\n",
            "Epoch 159/299\n",
            "----------\n",
            "Epoch Loss:  1304.2852890137583\n",
            "\n",
            "Epoch 160/299\n",
            "----------\n",
            "Epoch Loss:  1304.206303903833\n",
            "\n",
            "Epoch 161/299\n",
            "----------\n",
            "Epoch Loss:  1304.1276515983045\n",
            "\n",
            "Epoch 162/299\n",
            "----------\n",
            "Epoch Loss:  1304.0493632834405\n",
            "\n",
            "Epoch 163/299\n",
            "----------\n",
            "Epoch Loss:  1303.9713926110417\n",
            "\n",
            "Epoch 164/299\n",
            "----------\n",
            "Epoch Loss:  1303.893746484071\n",
            "\n",
            "Epoch 165/299\n",
            "----------\n",
            "Epoch Loss:  1303.8164252433926\n",
            "\n",
            "Epoch 166/299\n",
            "----------\n",
            "Epoch Loss:  1303.7394094932824\n",
            "\n",
            "Epoch 167/299\n",
            "----------\n",
            "Epoch Loss:  1303.6627426184714\n",
            "\n",
            "Epoch 168/299\n",
            "----------\n",
            "Epoch Loss:  1303.5863754414022\n",
            "\n",
            "Epoch 169/299\n",
            "----------\n",
            "Epoch Loss:  1303.5102987624705\n",
            "\n",
            "Epoch 170/299\n",
            "----------\n",
            "Epoch Loss:  1303.4345539081842\n",
            "\n",
            "Epoch 171/299\n",
            "----------\n",
            "Epoch Loss:  1303.3590808846056\n",
            "\n",
            "Epoch 172/299\n",
            "----------\n",
            "Epoch Loss:  1303.2839173823595\n",
            "\n",
            "Epoch 173/299\n",
            "----------\n",
            "Epoch Loss:  1303.209056723863\n",
            "\n",
            "Epoch 174/299\n",
            "----------\n",
            "Epoch Loss:  1303.1344717312604\n",
            "\n",
            "Epoch 175/299\n",
            "----------\n",
            "Epoch Loss:  1303.0601673536003\n",
            "\n",
            "Epoch 176/299\n",
            "----------\n",
            "Epoch Loss:  1302.9861913099885\n",
            "\n",
            "Epoch 177/299\n",
            "----------\n",
            "Epoch Loss:  1302.912403345108\n",
            "\n",
            "Epoch 178/299\n",
            "----------\n",
            "Epoch Loss:  1302.8390103187412\n",
            "\n",
            "Epoch 179/299\n",
            "----------\n",
            "Epoch Loss:  1302.7658166456968\n",
            "\n",
            "Epoch 180/299\n",
            "----------\n",
            "Epoch Loss:  1302.6929055992514\n",
            "\n",
            "Epoch 181/299\n",
            "----------\n",
            "Epoch Loss:  1302.620272114873\n",
            "\n",
            "Epoch 182/299\n",
            "----------\n",
            "Epoch Loss:  1302.5479263532907\n",
            "\n",
            "Epoch 183/299\n",
            "----------\n",
            "Epoch Loss:  1302.4757819846272\n",
            "\n",
            "Epoch 184/299\n",
            "----------\n",
            "Epoch Loss:  1302.40397144109\n",
            "\n",
            "Epoch 185/299\n",
            "----------\n",
            "Epoch Loss:  1302.3323586862534\n",
            "\n",
            "Epoch 186/299\n",
            "----------\n",
            "Epoch Loss:  1302.2610384579748\n",
            "\n",
            "Epoch 187/299\n",
            "----------\n",
            "Epoch Loss:  1302.1899535451084\n",
            "\n",
            "Epoch 188/299\n",
            "----------\n",
            "Epoch Loss:  1302.1191454585642\n",
            "\n",
            "Epoch 189/299\n",
            "----------\n",
            "Epoch Loss:  1302.0485128201544\n",
            "\n",
            "Epoch 190/299\n",
            "----------\n",
            "Epoch Loss:  1301.9782108869404\n",
            "\n",
            "Epoch 191/299\n",
            "----------\n",
            "Epoch Loss:  1301.908103376627\n",
            "\n",
            "Epoch 192/299\n",
            "----------\n",
            "Epoch Loss:  1301.8382545579225\n",
            "\n",
            "Epoch 193/299\n",
            "----------\n",
            "Epoch Loss:  1301.768622174859\n",
            "\n",
            "Epoch 194/299\n",
            "----------\n",
            "Epoch Loss:  1301.6992279533297\n",
            "\n",
            "Epoch 195/299\n",
            "----------\n",
            "Epoch Loss:  1301.6300935391337\n",
            "\n",
            "Epoch 196/299\n",
            "----------\n",
            "Epoch Loss:  1301.561160331592\n",
            "\n",
            "Epoch 197/299\n",
            "----------\n",
            "Epoch Loss:  1301.492488188669\n",
            "\n",
            "Epoch 198/299\n",
            "----------\n",
            "Epoch Loss:  1301.423983618617\n",
            "\n",
            "Epoch 199/299\n",
            "----------\n",
            "Epoch Loss:  1301.3557649329305\n",
            "\n",
            "Epoch 200/299\n",
            "----------\n",
            "Epoch Loss:  1301.28772444278\n",
            "\n",
            "Epoch 201/299\n",
            "----------\n",
            "Epoch Loss:  1301.2199435587972\n",
            "\n",
            "Epoch 202/299\n",
            "----------\n",
            "Epoch Loss:  1301.1523439828306\n",
            "\n",
            "Epoch 203/299\n",
            "----------\n",
            "Epoch Loss:  1301.0850145276636\n",
            "\n",
            "Epoch 204/299\n",
            "----------\n",
            "Epoch Loss:  1301.0177984368056\n",
            "\n",
            "Epoch 205/299\n",
            "----------\n",
            "Epoch Loss:  1300.9508781153709\n",
            "\n",
            "Epoch 206/299\n",
            "----------\n",
            "Epoch Loss:  1300.8841416481882\n",
            "\n",
            "Epoch 207/299\n",
            "----------\n",
            "Epoch Loss:  1300.8176170662045\n",
            "\n",
            "Epoch 208/299\n",
            "----------\n",
            "Epoch Loss:  1300.7512812651694\n",
            "\n",
            "Epoch 209/299\n",
            "----------\n",
            "Epoch Loss:  1300.6852093748748\n",
            "\n",
            "Epoch 210/299\n",
            "----------\n",
            "Epoch Loss:  1300.6192515920848\n",
            "\n",
            "Epoch 211/299\n",
            "----------\n",
            "Epoch Loss:  1300.5535797327757\n",
            "\n",
            "Epoch 212/299\n",
            "----------\n",
            "Epoch Loss:  1300.4880424756557\n",
            "\n",
            "Epoch 213/299\n",
            "----------\n",
            "Epoch Loss:  1300.4227342270315\n",
            "\n",
            "Epoch 214/299\n",
            "----------\n",
            "Epoch Loss:  1300.3575849216431\n",
            "\n",
            "Epoch 215/299\n",
            "----------\n",
            "Epoch Loss:  1300.292684983462\n",
            "\n",
            "Epoch 216/299\n",
            "----------\n",
            "Epoch Loss:  1300.2279535792768\n",
            "\n",
            "Epoch 217/299\n",
            "----------\n",
            "Epoch Loss:  1300.1633952520788\n",
            "\n",
            "Epoch 218/299\n",
            "----------\n",
            "Epoch Loss:  1300.0990498084575\n",
            "\n",
            "Epoch 219/299\n",
            "----------\n",
            "Epoch Loss:  1300.0348929259926\n",
            "\n",
            "Epoch 220/299\n",
            "----------\n",
            "Epoch Loss:  1299.9709237106144\n",
            "\n",
            "Epoch 221/299\n",
            "----------\n",
            "Epoch Loss:  1299.9071091078222\n",
            "\n",
            "Epoch 222/299\n",
            "----------\n",
            "Epoch Loss:  1299.843505229801\n",
            "\n",
            "Epoch 223/299\n",
            "----------\n",
            "Epoch Loss:  1299.7800969462842\n",
            "\n",
            "Epoch 224/299\n",
            "----------\n",
            "Epoch Loss:  1299.716842358932\n",
            "\n",
            "Epoch 225/299\n",
            "----------\n",
            "Epoch Loss:  1299.6537884492427\n",
            "\n",
            "Epoch 226/299\n",
            "----------\n",
            "Epoch Loss:  1299.5908858813345\n",
            "\n",
            "Epoch 227/299\n",
            "----------\n",
            "Epoch Loss:  1299.5281544215977\n",
            "\n",
            "Epoch 228/299\n",
            "----------\n",
            "Epoch Loss:  1299.4656376149505\n",
            "\n",
            "Epoch 229/299\n",
            "----------\n",
            "Epoch Loss:  1299.4032620191574\n",
            "\n",
            "Epoch 230/299\n",
            "----------\n",
            "Epoch Loss:  1299.3410679735243\n",
            "\n",
            "Epoch 231/299\n",
            "----------\n",
            "Epoch Loss:  1299.2790468223393\n",
            "\n",
            "Epoch 232/299\n",
            "----------\n",
            "Epoch Loss:  1299.2171983793378\n",
            "\n",
            "Epoch 233/299\n",
            "----------\n",
            "Epoch Loss:  1299.1555065400898\n",
            "\n",
            "Epoch 234/299\n",
            "----------\n",
            "Epoch Loss:  1299.0939922966063\n",
            "\n",
            "Epoch 235/299\n",
            "----------\n",
            "Epoch Loss:  1299.03266088292\n",
            "\n",
            "Epoch 236/299\n",
            "----------\n",
            "Epoch Loss:  1298.9714712426066\n",
            "\n",
            "Epoch 237/299\n",
            "----------\n",
            "Epoch Loss:  1298.9104760549963\n",
            "\n",
            "Epoch 238/299\n",
            "----------\n",
            "Epoch Loss:  1298.8495835848153\n",
            "\n",
            "Epoch 239/299\n",
            "----------\n",
            "Epoch Loss:  1298.7889128550887\n",
            "\n",
            "Epoch 240/299\n",
            "----------\n",
            "Epoch Loss:  1298.7283754032105\n",
            "\n",
            "Epoch 241/299\n",
            "----------\n",
            "Epoch Loss:  1298.6680033355951\n",
            "\n",
            "Epoch 242/299\n",
            "----------\n",
            "Epoch Loss:  1298.6077886652201\n",
            "\n",
            "Epoch 243/299\n",
            "----------\n",
            "Epoch Loss:  1298.5477138273418\n",
            "\n",
            "Epoch 244/299\n",
            "----------\n",
            "Epoch Loss:  1298.4878126941621\n",
            "\n",
            "Epoch 245/299\n",
            "----------\n",
            "Epoch Loss:  1298.4280753862113\n",
            "\n",
            "Epoch 246/299\n",
            "----------\n",
            "Epoch Loss:  1298.3684784509242\n",
            "\n",
            "Epoch 247/299\n",
            "----------\n",
            "Epoch Loss:  1298.309047471732\n",
            "\n",
            "Epoch 248/299\n",
            "----------\n",
            "Epoch Loss:  1298.2497704382986\n",
            "\n",
            "Epoch 249/299\n",
            "----------\n",
            "Epoch Loss:  1298.1906381323934\n",
            "\n",
            "Epoch 250/299\n",
            "----------\n",
            "Epoch Loss:  1298.1316443644464\n",
            "\n",
            "Epoch 251/299\n",
            "----------\n",
            "Epoch Loss:  1298.0728080365807\n",
            "\n",
            "Epoch 252/299\n",
            "----------\n",
            "Epoch Loss:  1298.0141577143222\n",
            "\n",
            "Epoch 253/299\n",
            "----------\n",
            "Epoch Loss:  1297.9555904828012\n",
            "\n",
            "Epoch 254/299\n",
            "----------\n",
            "Epoch Loss:  1297.8972326889634\n",
            "\n",
            "Epoch 255/299\n",
            "----------\n",
            "Epoch Loss:  1297.83896298334\n",
            "\n",
            "Epoch 256/299\n",
            "----------\n",
            "Epoch Loss:  1297.7808750141412\n",
            "\n",
            "Epoch 257/299\n",
            "----------\n",
            "Epoch Loss:  1297.7229263018817\n",
            "\n",
            "Epoch 258/299\n",
            "----------\n",
            "Epoch Loss:  1297.6651203520596\n",
            "\n",
            "Epoch 259/299\n",
            "----------\n",
            "Epoch Loss:  1297.6074399072677\n",
            "\n",
            "Epoch 260/299\n",
            "----------\n",
            "Epoch Loss:  1297.5499023161829\n",
            "\n",
            "Epoch 261/299\n",
            "----------\n",
            "Epoch Loss:  1297.4925119914114\n",
            "\n",
            "Epoch 262/299\n",
            "----------\n",
            "Epoch Loss:  1297.435242883861\n",
            "\n",
            "Epoch 263/299\n",
            "----------\n",
            "Epoch Loss:  1297.378144852817\n",
            "\n",
            "Epoch 264/299\n",
            "----------\n",
            "Epoch Loss:  1297.3211867623031\n",
            "\n",
            "Epoch 265/299\n",
            "----------\n",
            "Epoch Loss:  1297.264366997406\n",
            "\n",
            "Epoch 266/299\n",
            "----------\n",
            "Epoch Loss:  1297.2076116967946\n",
            "\n",
            "Epoch 267/299\n",
            "----------\n",
            "Epoch Loss:  1297.1510833092034\n",
            "\n",
            "Epoch 268/299\n",
            "----------\n",
            "Epoch Loss:  1297.094649573788\n",
            "\n",
            "Epoch 269/299\n",
            "----------\n",
            "Epoch Loss:  1297.0383471772075\n",
            "\n",
            "Epoch 270/299\n",
            "----------\n",
            "Epoch Loss:  1296.9821979161352\n",
            "\n",
            "Epoch 271/299\n",
            "----------\n",
            "Epoch Loss:  1296.926223313436\n",
            "\n",
            "Epoch 272/299\n",
            "----------\n",
            "Epoch Loss:  1296.8702875934541\n",
            "\n",
            "Epoch 273/299\n",
            "----------\n",
            "Epoch Loss:  1296.814531005919\n",
            "\n",
            "Epoch 274/299\n",
            "----------\n",
            "Epoch Loss:  1296.7589092571288\n",
            "\n",
            "Epoch 275/299\n",
            "----------\n",
            "Epoch Loss:  1296.70340333879\n",
            "\n",
            "Epoch 276/299\n",
            "----------\n",
            "Epoch Loss:  1296.6480612680316\n",
            "\n",
            "Epoch 277/299\n",
            "----------\n",
            "Epoch Loss:  1296.5927610043436\n",
            "\n",
            "Epoch 278/299\n",
            "----------\n",
            "Epoch Loss:  1296.537707284093\n",
            "\n",
            "Epoch 279/299\n",
            "----------\n",
            "Epoch Loss:  1296.4827353488654\n",
            "\n",
            "Epoch 280/299\n",
            "----------\n",
            "Epoch Loss:  1296.427844638005\n",
            "\n",
            "Epoch 281/299\n",
            "----------\n",
            "Epoch Loss:  1296.3731174897403\n",
            "\n",
            "Epoch 282/299\n",
            "----------\n",
            "Epoch Loss:  1296.318518312648\n",
            "\n",
            "Epoch 283/299\n",
            "----------\n",
            "Epoch Loss:  1296.264011517167\n",
            "\n",
            "Epoch 284/299\n",
            "----------\n",
            "Epoch Loss:  1296.2096797209233\n",
            "\n",
            "Epoch 285/299\n",
            "----------\n",
            "Epoch Loss:  1296.1554499790072\n",
            "\n",
            "Epoch 286/299\n",
            "----------\n",
            "Epoch Loss:  1296.1013246811926\n",
            "\n",
            "Epoch 287/299\n",
            "----------\n",
            "Epoch Loss:  1296.0473685022444\n",
            "\n",
            "Epoch 288/299\n",
            "----------\n",
            "Epoch Loss:  1295.9934813380241\n",
            "\n",
            "Epoch 289/299\n",
            "----------\n",
            "Epoch Loss:  1295.939779613167\n",
            "\n",
            "Epoch 290/299\n",
            "----------\n",
            "Epoch Loss:  1295.8861569873989\n",
            "\n",
            "Epoch 291/299\n",
            "----------\n",
            "Epoch Loss:  1295.832633189857\n",
            "\n",
            "Epoch 292/299\n",
            "----------\n",
            "Epoch Loss:  1295.779248379171\n",
            "\n",
            "Epoch 293/299\n",
            "----------\n",
            "Epoch Loss:  1295.7260167170316\n",
            "\n",
            "Epoch 294/299\n",
            "----------\n",
            "Epoch Loss:  1295.6728837899864\n",
            "\n",
            "Epoch 295/299\n",
            "----------\n",
            "Epoch Loss:  1295.6198375504464\n",
            "\n",
            "Epoch 296/299\n",
            "----------\n",
            "Epoch Loss:  1295.5669334735721\n",
            "\n",
            "Epoch 297/299\n",
            "----------\n",
            "Epoch Loss:  1295.5141679849476\n",
            "\n",
            "Epoch 298/299\n",
            "----------\n",
            "Epoch Loss:  1295.4615047257394\n",
            "\n",
            "Epoch 299/299\n",
            "----------\n",
            "Epoch Loss:  1295.4089628346264\n",
            "\n",
            "Training complete in 20m 34s\n",
            "End\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SphaZTC-IDDn"
      },
      "source": [
        "# Testing model 3 on all three test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hl4h8aWQ-iSg",
        "outputId": "4f666938-8bb4-4599-b953-80b2c8e15703"
      },
      "source": [
        "model = coherenceModel()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "coherenceModel(\n",
              "  (conv2DSent1): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent2): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (conv2DSent3): Conv2d(1, 16, kernel_size=(3, 50), stride=(1, 1), padding=(1, 0))\n",
              "  (avgPool2DSent1): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent2): AdaptiveAvgPool1d(output_size=1)\n",
              "  (avgPool2DSent3): AdaptiveAvgPool1d(output_size=1)\n",
              "  (hiddenLayer): Conv1d(1, 1, kernel_size=(1,), stride=(1,))\n",
              "  (finalLayer): AvgPool1d(kernel_size=(48,), stride=(48,), padding=(0,))\n",
              "  (endSigmoid): Sigmoid()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5-2ZiGIFCgy",
        "outputId": "b1788f94-d45f-4385-a56d-f5634527cd02"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/accident/datasetNTSBTesting.txt\", \"rb\") as fp:   # Unpickling\n",
        "  datasetNTSBTesting = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelAccident\",0, datasetNTSBTesting)\n",
        "incoherentScores = test_model(model, \"coherenceModelAccident\",1, datasetNTSBTesting)\n",
        "\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  83.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCnXxM7MgEgY",
        "outputId": "f23a7d41-9f92-4726-ed88-4404cbe36a3e"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/separate/separate_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  separate_test_embeddings = pickle.load(fp)\n",
        "\n",
        "coherentScores = test_model(model, \"coherenceModelAccident\",0, separate_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelAccident\",1, separate_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  64.03571986251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDHAttSmglEx",
        "outputId": "a789f4b1-816e-444f-c614-1f1feb30a8c8"
      },
      "source": [
        "with open(\"/content/drive/My Drive/nlp/combined/combined_test_embeddings.txt\", \"rb\") as fp:   # Unpickling\n",
        "  combined_test_embeddings = pickle.load(fp)\n",
        "  \n",
        "coherentScores = test_model(model, \"coherenceModelAccident\",0, combined_test_embeddings)\n",
        "incoherentScores = test_model(model, \"coherenceModelAccident\",1, combined_test_embeddings)\n",
        "total = len(coherentScores)\n",
        "correct=0\n",
        "for i in range(total):\n",
        "  if coherentScores[i] >= incoherentScores[i]:\n",
        "    correct+=1\n",
        "accuracy = (correct/total) * 100\n",
        "print(\"Test accuracy: \", accuracy)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Testing\n",
            "Start Testing\n",
            "Test accuracy:  67.821\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJjdmZGNN_Ys"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}