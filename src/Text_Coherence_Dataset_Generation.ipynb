{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Coherence_Dataset_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/udbhav-chugh/TextCoherenceOnSocialMedia/blob/master/Text_Coherence_Dataset_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9Z_ScWhBM9z"
      },
      "source": [
        "# This notebook contains code to synthesize the two datasets whose details are mentioned in the report:\n",
        "\n",
        "\n",
        "1.   Where the Reddit posts and comments are considered separately\n",
        "2.   Where the Reddit Posts and top 10 comments are considered.\n",
        "\n",
        "Note to run this notebook successfully, make sure to have the redditDataset folder in your google drive root directory.\n",
        "\n",
        "Use the following link to download the zip dataset folder\n",
        "https://www.kaggle.com/ammar111/reddit-top-1000\n",
        "\n",
        "Then extract it, rename it to redditDataset and upload it on your drive before running the steps ahead.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAXAMhxyVA4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "564fe88d-4bfa-4b25-a6fa-dd91670e9f4c"
      },
      "source": [
        "!pip install kora\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from kora.selenium import wd\n",
        "from selenium.webdriver import ActionChains\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import random\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting kora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/23/1a/6891aa274c9107375f20e1ebca7455d7bf562791d05645bda737ae488225/kora-0.9.11-py3-none-any.whl (55kB)\n",
            "\r\u001b[K     |██████                          | 10kB 18.0MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 20kB 18.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 30kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 40kB 8.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 51kB 4.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from kora) (5.5.0)\n",
            "Collecting fastcore\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/80/a61e8b91e2f65b0b6fd1449008470545ad771d6041e529504c6e5df4d4e7/fastcore-1.3.6-py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (50.3.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (2.6.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.7.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (0.8.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (4.3.3)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython->kora) (1.0.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (20.4)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.6/dist-packages (from fastcore->kora) (19.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython->kora) (0.6.0)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (0.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2->ipython->kora) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->kora) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastcore->kora) (2.4.7)\n",
            "Installing collected packages: fastcore, kora\n",
            "Successfully installed fastcore-1.3.6 kora-0.9.11\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grADQi2vC0FK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824c5673-9094-4b0f-b97d-043ecae8a817"
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iZ_gnO0DOg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c88ab882-38b3-4229-dac9-16e113ddab60"
      },
      "source": [
        "#get file names from the redditDataset folder\n",
        "mypath = '/content/gdrive/My Drive/redditDataset'\n",
        "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n",
        "print(onlyfiles)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['gifs.csv', 'AskReddit.csv', 'Jokes.csv', 'Showerthoughts.csv', 'pics.csv', 'food.csv', 'worldnews.csv', 'woahdude.csv', 'aww.csv', 'funny.csv', 'books.csv', 'IAmA.csv', 'videos.csv', 'LifeProTips.csv', 'todayilearned.csv', 'GetMotivated.csv', 'movies.csv', 'explainlikeimfive.csv']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB5ZaZAeDkzR"
      },
      "source": [
        "#function to get urls listed in the csv file passed as parameter\n",
        "def getUrls(file):\n",
        "  urls = []\n",
        "  df = pd.read_csv(join(mypath, file))\n",
        "  # cols = df.columns.tolist()\n",
        "  urlCol = df['permalink']\n",
        "  # print(len(urlCol))\n",
        "  for key, value in urlCol.iteritems():\n",
        "    urls.append(value)\n",
        "  return urls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6r0bUlCyDIrB"
      },
      "source": [
        "# Type 1 Dataset: Posts and Comments considered separately"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIXJyuhoR_SZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970ab0f2-d14d-44a5-ca80-cb90fa994e77"
      },
      "source": [
        "fileToWrite = '/content/gdrive/My Drive/redditPostsAndCommentsSeparate.txt'\n",
        "with open(fileToWrite, 'w') as f:\n",
        "  f.write('')\n",
        "\n",
        "for file in ['AskReddit.csv','explainlikeimfive.csv','books.csv', 'Jokes.csv']:\n",
        "  urls = getUrls(file)\n",
        "  count=0\n",
        "  for url in urls:\n",
        "    if count >= 70:\n",
        "      break\n",
        "    wd.get(url)\n",
        "    wd.implicitly_wait(1000)\n",
        "    elem = wd.find_elements_by_xpath(\"//div[@data-click-id='text']\")\n",
        "    if(len(elem)==0):\n",
        "      continue\n",
        "\n",
        "    post=elem[0]\n",
        "    if(len(post.text) >= 1000):\n",
        "      count+=1\n",
        "      with open(fileToWrite, 'a') as f:\n",
        "        f.write(post.text.replace(\"\\n\", \" \")+'\\n')\n",
        "\n",
        "    comments = wd.find_elements_by_xpath(\"//div[@data-test-id='comment']\")\n",
        "    for comment in comments:\n",
        "      if(len(comment.text) >= 1000):\n",
        "        count+=1\n",
        "        with open(fileToWrite, 'a') as f:\n",
        "          f.write(comment.text.replace(\"\\n\", \" \")+'\\n')\n",
        "  print(\"Data extracted from \",file)\n",
        "\n",
        "print(\"Separate Post and Comment Data extracted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data extracted from  AskReddit.csv\n",
            "Data extracted from  explainlikeimfive.csv\n",
            "Data extracted from  books.csv\n",
            "Data extracted from  Jokes.csv\n",
            "Separate Post and Comment Data extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbRM4aQbDOik"
      },
      "source": [
        "# Type 2 Dataset: Posts and Top 10 Comments considered Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSDSGvyZoiAr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f22147c1-052d-4913-87e6-ec4653faa512"
      },
      "source": [
        "fileToWrite = '/content/gdrive/My Drive/redditPostsAndCommentsCombined.txt'\n",
        "\n",
        "with open(fileToWrite, 'w') as f:\n",
        "  f.write('')\n",
        "\n",
        "for file in ['AskReddit.csv','explainlikeimfive.csv','books.csv', 'Jokes.csv']:\n",
        "  urls = getUrls(file)\n",
        "  count=0\n",
        "  for url in urls:\n",
        "    if count >= 70:\n",
        "      break\n",
        "    wd.get(url)\n",
        "    wd.implicitly_wait(1000)\n",
        "    elem = wd.find_elements_by_xpath(\"//div[@data-click-id='text']\")\n",
        "    if(len(elem)==0):\n",
        "      continue\n",
        "\n",
        "    post=elem[0]\n",
        "    finalText = post.text\n",
        "    comments = wd.find_elements_by_xpath(\"//div[@data-test-id='comment']\")\n",
        "    commentCount = 0\n",
        "    for comment in comments:\n",
        "      commentCount +=1\n",
        "      if(commentCount>10):\n",
        "        break\n",
        "      finalText = finalText + comment.text\n",
        "\n",
        "    if(len(finalText) >= 1000):\n",
        "      count+=1\n",
        "      with open(fileToWrite, 'a') as f:\n",
        "        f.write(finalText.replace(\"\\n\", \" \")+'\\n')\n",
        "\n",
        "  print(\"Data extracted from \",file)\n",
        "\n",
        "print(\"Combined Post and Comment Data extracted\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data extracted from  AskReddit.csv\n",
            "Data extracted from  explainlikeimfive.csv\n",
            "Data extracted from  books.csv\n",
            "Data extracted from  Jokes.csv\n",
            "Combined Post and Comment Data extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6hQnelTCm8I"
      },
      "source": [
        "#Function to receive a text input, return list of permutations of the original text which can later be paired up with original text while tra \n",
        "def getPermutations(text):\n",
        "  tokenizedText = sent_tokenize(text)\n",
        "  permutedTexts=[]\n",
        "  for i in range(20):\n",
        "    random.shuffle(tokenizedText)\n",
        "    result = ''\n",
        "    for sentence in tokenizedText:\n",
        "      result += sentence\n",
        "      result += ' '\n",
        "    permutedTexts.append(result)\n",
        "  \n",
        "  return permutedTexts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMSJinaXWcbf"
      },
      "source": [
        "def getDataset(redditPostsAndComments, finalDatasetFile):\n",
        "  with open(redditPostsAndComments, 'r') as f:\n",
        "    redditPosts = f.readlines()\n",
        "\n",
        "  with open(finalDatasetFile, 'w') as f:\n",
        "    f.write('')\n",
        "\n",
        "  for post in redditPosts:\n",
        "    post = post.replace(\"\\t\", \" \")\n",
        "    post = post.replace(\"\\n\", \" \")\n",
        "    permutedPosts = getPermutations(post)\n",
        "    for permutedPost in permutedPosts:\n",
        "      dataPair = post + \"\\t\" + permutedPost\n",
        "      with open(finalDatasetFile, 'a') as f:\n",
        "        f.write(dataPair+\"\\n\")\n",
        "  print(\"Dataset file generated successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_qsnhEQXGgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63955817-a8cb-4e11-df1f-905ca120c2f8"
      },
      "source": [
        "#get type 1 dataset\n",
        "getDataset('/content/gdrive/My Drive/redditPostsAndCommentsSeparate.txt', '/content/gdrive/My Drive/datasetPostsAndCommentsSeparate.tsv')\n",
        "\n",
        "#get type 2 dataset\n",
        "getDataset('/content/gdrive/My Drive/redditPostsAndCommentsCombined.txt', '/content/gdrive/My Drive/datasetPostsAndCommentsCombined.tsv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset file generated successfully.\n",
            "Dataset file generated successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM4Oo-KY9awc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29530d21-7d17-4429-e6eb-3c253062cc2e"
      },
      "source": [
        "#get training accident dataset\n",
        "getDataset('/content/gdrive/My Drive/trainingAccidents.txt', '/content/gdrive/My Drive/datasetTrainingAccidents.tsv')\n",
        "\n",
        "#get testing accident dataset\n",
        "getDataset('/content/gdrive/My Drive/testingAccidents.txt', '/content/gdrive/My Drive/datasetTestingAccidents.tsv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset file generated successfully.\n",
            "Dataset file generated successfully.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5A2Ibb0N0x6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}